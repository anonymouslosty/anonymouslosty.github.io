

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/Github.png">
  <link rel="icon" href="/img/Github.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Ling yi">
  <meta name="keywords" content="">
  
    <meta name="description" content="教程 https:&#x2F;&#x2F;zh-v2.d2l.ai&#x2F;chapter_preface&#x2F;index.html">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习 循环神经网络 笔记">
<meta property="og:url" content="https://anonymouslosty.github.io/2023/07/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="anonymouslosty的Blog">
<meta property="og:description" content="教程 https:&#x2F;&#x2F;zh-v2.d2l.ai&#x2F;chapter_preface&#x2F;index.html">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://anonymouslosty.github.io/img/RNN_1.png">
<meta property="og:image" content="https://anonymouslosty.github.io/img/%E6%A8%A1%E5%9E%8B2.png">
<meta property="og:image" content="https://anonymouslosty.github.io/img/%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B.png">
<meta property="og:image" content="https://anonymouslosty.github.io/img/%E6%BD%9C%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B.png">
<meta property="og:image" content="https://anonymouslosty.github.io/img/%E6%BD%9C%E5%8F%98%E9%87%8F%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B.png">
<meta property="og:image" content="https://anonymouslosty.github.io/img/RNN_init.jpg">
<meta property="article:published_time" content="2023-07-05T02:34:53.000Z">
<meta property="article:modified_time" content="2023-07-23T14:02:36.244Z">
<meta property="article:author" content="Ling yi">
<meta property="article:tag" content="DeepLearning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://anonymouslosty.github.io/img/RNN_1.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>深度学习 循环神经网络 笔记 - anonymouslosty的Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"anonymouslosty.github.io","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":90,"cursorChar":"..","loop":true,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4","placement":"left","visible":"hover","icon":"#"},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":true,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"left","headingSelector":"h1,h2,h3,h4","collapseDepth":2},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":true}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 80vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>anonymouslosty的Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/band_deer.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="深度学习 循环神经网络 笔记"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Ling yi
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-07-05 10:34" pubdate>
          2023年7月5日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          18k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          150 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
        <div class="scroll-down-bar">
          <i class="iconfont icon-arrowdown"></i>
        </div>
      
    </div>
  </div>
</div>

</div>

    <a target="_blank" rel="noopener" href="https://github.com/anonymouslosty" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#fff; color:#151513; position: absolute; top: 0; border: 0; right: 0; z-index: 1031;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="padding-left: 2rem; margin-right: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">深度学习 循环神经网络 笔记</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：2023年7月23日 晚上
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <h1 id="序列模型">序列模型</h1>
<h2 id="数据">数据</h2>
<ul>
<li>现实世界中很多数据是有时序结构的</li>
<li>音乐、语言、文本，很多数据都是连续的</li>
</ul>
<h2 id="统计工具">统计工具</h2>
<ul>
<li>不独立的随机变量
<ul>
<li>这是时序序列和其他序列的主要区别</li>
</ul></li>
</ul>
<figure>
<img src="/img/RNN_1.png" srcset="/img/loading.gif" lazyload alt="模型" />
<figcaption aria-hidden="true">模型</figcaption>
</figure>
<figure>
<img src="/img/模型2.png" srcset="/img/loading.gif" lazyload alt="模型2" />
<figcaption aria-hidden="true">模型2</figcaption>
</figure>
<h2 id="序列模型-1">序列模型</h2>
<ul>
<li>对条件概率建模，自回归模型</li>
<li>给一定数据，预测另外一组数据时用前面给的数据</li>
<li>核心：计算<span class="math inline">\(P\)</span>、<span
class="math inline">\(f(x_1,...x_{t-1})\)</span></li>
</ul>
<figure>
<img src="/img/自回归模型.png" srcset="/img/loading.gif" lazyload alt="自回归模型" />
<figcaption aria-hidden="true">自回归模型</figcaption>
</figure>
<h4 id="马尔科夫假设">马尔科夫假设</h4>
<ul>
<li>假设当前数据只跟<span
class="math inline">\(\tau\)</span>个过去数据点相关</li>
</ul>
<p>​ <span
class="math display">\[p(x_t|x_1,...x_{t-1})=p(x_t|x_{t-\tau},...x_{t-1})=p(x_t|f(x_{t-\tau},...x_{t-1}))\]</span></p>
<h4 id="潜变量模型">潜变量模型</h4>
<ul>
<li>引入潜变量<span
class="math inline">\(h_t\)</span>来表示过去信息<span
class="math inline">\(h_t=f(x_1,...x_{t-1})\)</span>
<ul>
<li><span class="math inline">\(x_t = p(x_t|h_t)\)</span></li>
<li>新的<span class="math inline">\(h&#39;\)</span>和前面的<span
class="math inline">\(h\)</span>和<span
class="math inline">\(x\)</span>相关（模型1）</li>
<li>给定前面的<span class="math inline">\(x\)</span>和新的<span
class="math inline">\(h&#39;\)</span>计算新的<span
class="math inline">\(x&#39;\)</span>（模型2）</li>
</ul></li>
</ul>
<p>　<img src="/img/潜变量模型.png" srcset="/img/loading.gif" lazyload alt="潜变量模型" /></p>
<h4 id="潜变量自回归模型">潜变量自回归模型</h4>
<figure>
<img src="/img/潜变量自回归模型.png" srcset="/img/loading.gif" lazyload alt="潜变量自回归模型" />
<figcaption aria-hidden="true">潜变量自回归模型</figcaption>
</figure>
<ul>
<li>使用潜变量<span class="math inline">\(h_t\)</span>总结过去信息</li>
</ul>
<h4 id="总结">总结</h4>
<ul>
<li>时序模型中，当前数据只跟前面观察到的数据相关</li>
<li>自回归模型使用自身过去数据来预测未来</li>
<li>马尔可夫模型假设当前只跟最近少数数据相关，从而简化模型</li>
<li>潜变量模型使用潜变量来概括历史信息</li>
</ul>
<h4 id="代码">代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>T = <span class="hljs-number">1000</span>  <span class="hljs-comment"># 总共产生1000个点</span><br>time = torch.arange(<span class="hljs-number">1</span>, T + <span class="hljs-number">1</span>, dtype=torch.float32)<br>x = torch.sin(<span class="hljs-number">0.01</span> * time) + torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.2</span>, (T,))<br>d2l.plot(time, [x], <span class="hljs-string">&#x27;time&#x27;</span>, <span class="hljs-string">&#x27;x&#x27;</span>, xlim=[<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>], figsize=(<span class="hljs-number">6</span>, <span class="hljs-number">3</span>))<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">tau = <span class="hljs-number">4</span><br><span class="hljs-comment"># 996 4</span><br>features = torch.zeros((T - tau, tau))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tau):<br>    features[:, i] = x[i: T - tau + i]<br>labels = x[tau:].reshape((-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br><br>batch_size, n_train = <span class="hljs-number">16</span>, <span class="hljs-number">600</span><br><span class="hljs-comment"># 只有前n_train个样本用于训练</span><br>train_iter = d2l.load_array((features[:n_train], labels[:n_train]),<br>                            batch_size, is_train=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 初始化网络权重的函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">m</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>        nn.init.xavier_uniform_(m.weight)<br><br><span class="hljs-comment"># 一个简单的多层感知机</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_net</span>():<br>    net = nn.Sequential(nn.Linear(<span class="hljs-number">4</span>, <span class="hljs-number">10</span>),<br>                        nn.ReLU(),<br>                        nn.Linear(<span class="hljs-number">10</span>, <span class="hljs-number">1</span>))<br>    net.apply(init_weights)<br>    <span class="hljs-keyword">return</span> net<br><br><span class="hljs-comment"># 平方损失。注意：MSELoss计算平方误差时不带系数1/2</span><br>loss = nn.MSELoss(reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">net, train_iter, loss, epochs, lr</span>):<br>    trainer = torch.optim.Adam(net.parameters(), lr)<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_iter:<br>            trainer.zero_grad()<br>            l = loss(net(X), y)<br>            l.<span class="hljs-built_in">sum</span>().backward()<br>            trainer.step()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;epoch <span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span>, &#x27;</span><br>              <span class="hljs-string">f&#x27;loss: <span class="hljs-subst">&#123;d2l.evaluate_loss(net, train_iter, loss):f&#125;</span>&#x27;</span>)<br><br>net = get_net()<br>train(net, train_iter, loss, <span class="hljs-number">5</span>, <span class="hljs-number">0.01</span>)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">onestep_preds = net(features)<br>d2l.plot([time, time[tau:]],<br>         [x.detach().numpy(), onestep_preds.detach().numpy()], <span class="hljs-string">&#x27;time&#x27;</span>,<br>         <span class="hljs-string">&#x27;x&#x27;</span>, legend=[<span class="hljs-string">&#x27;data&#x27;</span>, <span class="hljs-string">&#x27;1-step preds&#x27;</span>], xlim=[<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>],<br>         figsize=(<span class="hljs-number">6</span>, <span class="hljs-number">3</span>))<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">multistep_preds = torch.zeros(T)<br>multistep_preds[: n_train + tau] = x[: n_train + tau]<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_train + tau, T):<br>    multistep_preds[i] = net(<br>        multistep_preds[i - tau:i].reshape((<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)))<br><br>d2l.plot([time, time[tau:], time[n_train + tau:]],<br>         [x.detach().numpy(), onestep_preds.detach().numpy(),<br>          multistep_preds[n_train + tau:].detach().numpy()], <span class="hljs-string">&#x27;time&#x27;</span>,<br>         <span class="hljs-string">&#x27;x&#x27;</span>, legend=[<span class="hljs-string">&#x27;data&#x27;</span>, <span class="hljs-string">&#x27;1-step preds&#x27;</span>, <span class="hljs-string">&#x27;multistep preds&#x27;</span>],<br>         xlim=[<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>], figsize=(<span class="hljs-number">6</span>, <span class="hljs-number">3</span>))<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">max_steps = <span class="hljs-number">64</span><br><br>features = torch.zeros((T - tau - max_steps + <span class="hljs-number">1</span>, tau + max_steps))<br><span class="hljs-comment"># 列i（i&lt;tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tau):<br>    features[:, i] = x[i: i + T - tau - max_steps + <span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># 列i（i&gt;=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tau, tau + max_steps):<br>    features[:, i] = net(features[:, i - tau:i]).reshape(-<span class="hljs-number">1</span>)<br><br>steps = (<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">16</span>, <span class="hljs-number">64</span>)<br>d2l.plot([time[tau + i - <span class="hljs-number">1</span>: T - max_steps + i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> steps],<br>         [features[:, tau + i - <span class="hljs-number">1</span>].detach().numpy() <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> steps], <span class="hljs-string">&#x27;time&#x27;</span>, <span class="hljs-string">&#x27;x&#x27;</span>,<br>         legend=[<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;i&#125;</span>-step preds&#x27;</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> steps], xlim=[<span class="hljs-number">5</span>, <span class="hljs-number">1000</span>],<br>         figsize=(<span class="hljs-number">6</span>, <span class="hljs-number">3</span>))<br></code></pre></td></tr></table></figure>
<h3 id="文本预处理">文本预处理</h3>
<h4 id="代码-1">代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> collections<br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 读取数据集</span><br><span class="hljs-comment">#@save</span><br>d2l.DATA_HUB[<span class="hljs-string">&#x27;time_machine&#x27;</span>] = (d2l.DATA_URL + <span class="hljs-string">&#x27;timemachine.txt&#x27;</span>,<br>                                <span class="hljs-string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_time_machine</span>():  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;将时间机器数据集加载到文本行的列表中&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(d2l.download(<span class="hljs-string">&#x27;time_machine&#x27;</span>), <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        lines = f.readlines()<br>    <span class="hljs-keyword">return</span> [re.sub(<span class="hljs-string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, line).strip().lower() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br><br>lines = read_time_machine()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;# 文本总行数: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(lines)&#125;</span>&#x27;</span>)<br><span class="hljs-built_in">print</span>(lines[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(lines[<span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 根据token=&#x27;word&#x27;或&#x27;char&#x27;</span><br><span class="hljs-comment"># 将句子拆分为一个个单词或者字符</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">lines, token=<span class="hljs-string">&#x27;word&#x27;</span></span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;将文本行拆分为单词或字符词元&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> token == <span class="hljs-string">&#x27;word&#x27;</span>:<br>        <span class="hljs-keyword">return</span> [line.split() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br>    <span class="hljs-keyword">elif</span> token == <span class="hljs-string">&#x27;char&#x27;</span>:<br>        <span class="hljs-keyword">return</span> [<span class="hljs-built_in">list</span>(line) <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;错误：未知词元类型：&#x27;</span> + token)<br><br>tokens = tokenize(lines)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">11</span>):<br>    <span class="hljs-built_in">print</span>(tokens[i])<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将tokens创建Vocabulary 词表</span><br><span class="hljs-comment"># 建立单词与索引的关系</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Vocab</span>:  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;文本词表&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, tokens=<span class="hljs-literal">None</span>, min_freq=<span class="hljs-number">0</span>, reserved_tokens=<span class="hljs-literal">None</span></span>):<br>      <span class="hljs-comment"># min_freq 出现频率小于该参数的单词忽略不计</span><br>        <span class="hljs-keyword">if</span> tokens <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            tokens = []<br>        <span class="hljs-keyword">if</span> reserved_tokens <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            reserved_tokens = []<br>        <span class="hljs-comment"># 按出现频率排序</span><br>        counter = count_corpus(tokens)<br>        self._token_freqs = <span class="hljs-built_in">sorted</span>(counter.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>],<br>                                   reverse=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># 未知词元的索引为0</span><br>        self.idx_to_token = [<span class="hljs-string">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens<br>        self.token_to_idx = &#123;token: idx<br>                             <span class="hljs-keyword">for</span> idx, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.idx_to_token)&#125;<br>        <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> self._token_freqs:<br>            <span class="hljs-keyword">if</span> freq &lt; min_freq:<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">if</span> token <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.token_to_idx:<br>                self.idx_to_token.append(token)<br>                self.token_to_idx[token] = <span class="hljs-built_in">len</span>(self.idx_to_token) - <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.idx_to_token)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, tokens</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(tokens, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            <span class="hljs-keyword">return</span> self.token_to_idx.get(tokens, self.unk)<br>        <span class="hljs-keyword">return</span> [self.__getitem__(token) <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> tokens]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">to_tokens</span>(<span class="hljs-params">self, indices</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(indices, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            <span class="hljs-keyword">return</span> self.idx_to_token[indices]<br>        <span class="hljs-keyword">return</span> [self.idx_to_token[index] <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> indices]<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">unk</span>(<span class="hljs-params">self</span>):  <span class="hljs-comment"># 未知词元的索引为0</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">token_freqs</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self._token_freqs<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">count_corpus</span>(<span class="hljs-params">tokens</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 这里的tokens是1D列表或2D列表</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(tokens) == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> <span class="hljs-built_in">isinstance</span>(tokens[<span class="hljs-number">0</span>], <span class="hljs-built_in">list</span>):<br>        <span class="hljs-comment"># 将词元列表展平成一个列表</span><br>        tokens = [token <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> line]<br>    <span class="hljs-keyword">return</span> collections.Counter(tokens)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_corpus_time_machine</span>(<span class="hljs-params">max_tokens=-<span class="hljs-number">1</span></span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;</span><br>    lines = read_time_machine()<br>    tokens = tokenize(lines, <span class="hljs-string">&#x27;char&#x27;</span>)<br>    vocab = Vocab(tokens)<br>    <span class="hljs-comment"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，</span><br>    <span class="hljs-comment"># 所以将所有文本行展平到一个列表中</span><br>    corpus = [vocab[token] <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> line]<br>    <span class="hljs-keyword">if</span> max_tokens &gt; <span class="hljs-number">0</span>:<br>        corpus = corpus[:max_tokens]<br>    <span class="hljs-keyword">return</span> corpus, vocab<br><br>corpus, vocab = load_corpus_time_machine()<br><span class="hljs-built_in">len</span>(corpus), <span class="hljs-built_in">len</span>(vocab)<br></code></pre></td></tr></table></figure>
<h3 id="section"></h3>
<h3 id="语言模型数据集">语言模型数据集</h3>
<h4 id="目的">目的</h4>
<p>对一个文档，甚至是一个词元序列进行建模。</p>
<p><span class="math display">\[P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T
P(x_t  \mid  x_1, \ldots, x_{t-1}).\]</span></p>
<p><span class="math display">\[P(\text{deep}, \text{learning},
\text{is}, \text{fun}) =  P(\text{deep})
P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep},
\text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning},
\text{is}).\]</span></p>
<h4 id="马尔可夫模型">马尔可夫模型</h4>
<ul>
<li>n元语法</li>
</ul>
<p>涉及一个、两个和三个变量的概率公式分别被称为
<em>一元语法</em>（unigram）、<em>二元语法</em>（bigram）和<em>三元语法</em>（trigram）模型。</p>
<p><span class="math display">\[\begin{split}\begin{aligned}P(x_1, x_2,
x_3, x_4) &amp;=  P(x_1) P(x_2) P(x_3) P(x_4),\\
P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_2)
P(x_4  \mid  x_3),\\
P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_1,
x_2) P(x_4  \mid  x_2, x_3).
\end{aligned}\end{split}\]</span></p>
<h4 id="读取长时间序列数据">读取长时间序列数据</h4>
<ul>
<li><strong>随机分区</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">seq_data_iter_random</span>(<span class="hljs-params">corpus, batch_size, num_steps</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;使用随机抽样生成一个小批量子序列&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1</span><br>    <span class="hljs-comment"># [4:34] len = 31</span><br>    corpus = corpus[random.randint(<span class="hljs-number">0</span>, num_steps - <span class="hljs-number">1</span>):]<br>    <span class="hljs-comment"># 减去1，是因为我们需要考虑标签</span><br>    <span class="hljs-comment"># 6个sebseq 每个包含5个词元</span><br>    num_subseqs = (<span class="hljs-built_in">len</span>(corpus) - <span class="hljs-number">1</span>) // num_steps<br>    <span class="hljs-comment"># 长度为num_steps的子序列的起始索引</span><br>    <span class="hljs-comment"># [0,5,10,15,20,25]</span><br>    initial_indices = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_subseqs * num_steps, num_steps))<br>    <span class="hljs-comment"># 在随机抽样的迭代过程中，</span><br>    <span class="hljs-comment"># 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span><br>    <span class="hljs-comment"># [5,15,0,25,10,20]</span><br>    random.shuffle(initial_indices)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">data</span>(<span class="hljs-params">pos</span>):<br>        <span class="hljs-comment"># 返回从pos位置开始的长度为num_steps的序列</span><br>        <span class="hljs-keyword">return</span> corpus[pos: pos + num_steps]<br><br>    num_batches = num_subseqs // batch_size<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, batch_size * num_batches, batch_size):<br>        <span class="hljs-comment"># 在这里，initial_indices包含子序列的随机起始索引</span><br>        <span class="hljs-comment"># [5,15] [0,25] [10,20]</span><br>        initial_indices_per_batch = initial_indices[i: i + batch_size]<br>        <span class="hljs-comment"># [5:5+5],[15:15+5] ...</span><br>        X = [data(j) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> initial_indices_per_batch]<br>        <span class="hljs-comment"># [6:6+5],[16:16+5] ... </span><br>        Y = [data(j + <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> initial_indices_per_batch]<br>        <span class="hljs-keyword">yield</span> torch.tensor(X), torch.tensor(Y)<br>        <br>seg_data_iter_random(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">35</span>),bathch_size=<span class="hljs-number">2</span>,num_steps=<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure>
<ul>
<li>顺序分区</li>
</ul>
<p>​ 没有<code>random.shuffle</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">seq_data_iter_sequential</span>(<span class="hljs-params">corpus, batch_size, num_steps</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;使用顺序分区生成一个小批量子序列&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 从随机偏移量开始划分序列</span><br>    offset = random.randint(<span class="hljs-number">0</span>, num_steps)<br>    num_tokens = ((<span class="hljs-built_in">len</span>(corpus) - offset - <span class="hljs-number">1</span>) // batch_size) * batch_size<br>    Xs = torch.tensor(corpus[offset: offset + num_tokens])<br>    Ys = torch.tensor(corpus[offset + <span class="hljs-number">1</span>: offset + <span class="hljs-number">1</span> + num_tokens])<br>    Xs, Ys = Xs.reshape(batch_size, -<span class="hljs-number">1</span>), Ys.reshape(batch_size, -<span class="hljs-number">1</span>)<br>    num_batches = Xs.shape[<span class="hljs-number">1</span>] // num_steps<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_steps * num_batches, num_steps):<br>        X = Xs[:, i: i + num_steps]<br>        Y = Ys[:, i: i + num_steps]<br>        <span class="hljs-keyword">yield</span> X, Y<br></code></pre></td></tr></table></figure>
<ul>
<li><strong>封装</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SeqDataLoader</span>:  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;加载序列数据的迭代器&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, batch_size, num_steps, use_random_iter, max_tokens</span>):<br>        <span class="hljs-keyword">if</span> use_random_iter:<br>            self.data_iter_fn = d2l.seq_data_iter_random<br>        <span class="hljs-keyword">else</span>:<br>            self.data_iter_fn = d2l.seq_data_iter_sequential<br>        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)<br>        self.batch_size, self.num_steps = batch_size, num_steps<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__iter__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)<br>      <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data_time_machine</span>(<span class="hljs-params">batch_size, num_steps,  <span class="hljs-comment">#@save</span></span><br><span class="hljs-params">                           use_random_iter=<span class="hljs-literal">False</span>, max_tokens=<span class="hljs-number">10000</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;返回时光机器数据集的迭代器和词表&quot;&quot;&quot;</span><br>    data_iter = SeqDataLoader(<br>        batch_size, num_steps, use_random_iter, max_tokens)<br>    <span class="hljs-keyword">return</span> data_iter, data_iter.vocab<br></code></pre></td></tr></table></figure>
<h2 id="循环神经网络">循环神经网络</h2>
<h4 id="独热编码">独热编码</h4>
<h5 id="概念">概念</h5>
<ul>
<li>将词元的数字索引变为更具表现力的特征向量</li>
</ul>
<h5 id="方法">方法</h5>
<ul>
<li><p>假设词表中不同词元的数目为<span
class="math inline">\(N\)</span>(<code>len(vocab)</code>)，词元索引的范围为<span
class="math inline">\(0\)</span>到<span
class="math inline">\(N-1\)</span></p></li>
<li><p>如果词元的索引是整数<span class="math inline">\(i\)</span>，
那么我们将创建一个长度为<span class="math inline">\(N\)</span>的全向量，
并将第处的元素设置为<span class="math inline">\(i\)</span>。</p></li>
</ul>
<h5 id="注意">注意</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># (2,5) batch_size = 2 num_step = 5 (一次五个字符(词元),时间维度、长度T )</span><br>X = torch.arange(<span class="hljs-number">10</span>).reshape((<span class="hljs-number">2</span>, <span class="hljs-number">5</span>))<br><span class="hljs-comment"># X.T 把时间维度提到第一维</span><br><span class="hljs-comment"># 因为后面是对时间维度进行更新</span><br><span class="hljs-comment"># X.T更方便</span><br>F.one_hot(X.T, <span class="hljs-number">28</span>).shape<br><br><span class="hljs-comment"># [5,2,28]</span><br></code></pre></td></tr></table></figure>
<h4 id="梯度裁剪">梯度裁剪</h4>
<h5 id="原因">原因</h5>
<p>对于长度为的序列，我们在迭代中计算这个时间步上的梯度，
将会在反向传播过程中产生长度为的矩阵乘法链。 当<strong><span
class="math inline">\(\mathcal{O}(T)\)</span></strong>较大时，它可能导致数值不稳定，
例如可能导致梯度爆炸或梯度消失。
因此，循环神经网络模型往往需要额外的方式来支持稳定训练。</p>
<h5 id="概念-1">概念</h5>
<ul>
<li>如果梯度长度超过<span
class="math inline">\(\theta\)</span>，那么拖影回长度$ $</li>
</ul>
<p>​ <span class="math display">\[\mathbf{g} \leftarrow \min\left(1,
\frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}.\]</span></p>
<h4 id="困惑度">困惑度</h4>
<h5 id="概念-2">概念</h5>
<ul>
<li>下一个词元的实际选择数的调和平均数</li>
<li>用于衡量语言模型的质量</li>
<li>我们在引入softmax回归 （ <a
target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_linear-networks/softmax-regression.html#subsec-info-theory-basics">3.4.7节</a>）时定义了熵、惊异和交叉熵。
如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。
一个更好的语言模型应该能让我们更准确地预测下一个词元。
因此，它应该允许我们在压缩序列时花费更少的比特。
所以我们可以通过一个序列中所有的个词元的交叉熵损失的平均值来衡量：</li>
</ul>
<p>​ <span class="math display">\[\frac{1}{n} \sum_{t=1}^n -\log P(x_t
\mid x_{t-1}, \ldots, x_1),\]</span></p>
<ul>
<li>其中由语言模型给出， 是在时间步从该序列中观察到的实际词元。
这使得不同长度的文档的性能具有了可比性。
由于历史原因，自然语言处理的科学家更喜欢使用一个叫做<em>困惑度</em>（perplexity）的量。
简而言之，它是 交叉熵损失的平均值的指数：</li>
</ul>
<p>​ <span class="math display">\[\exp\left(-\frac{1}{n} \sum_{t=1}^n
\log P(x_t \mid x_{t-1}, \ldots, x_1)\right).\]</span></p>
<ul>
<li>预测情况
<ul>
<li>在最好的情况下，模型总是完美地估计标签词元的概率为1。
在这种情况下，模型的困惑度为1。</li>
<li>在最坏的情况下，模型总是预测标签词元的概率为0。
在这种情况下，困惑度是正无穷大。</li>
<li>在基线上，该模型的预测是词表的所有可用词元上的均匀分布。
在这种情况下，困惑度等于词表中唯一词元的数量。
事实上，如果我们在没有任何压缩的情况下存储序列，
这将是我们能做的最好的编码方式。 因此，这种方式提供了一个重要的上限，
而任何实际模型都必须超越这个上限。</li>
</ul></li>
</ul>
<h3 id="实现部分">实现部分</h3>
<p><strong>一次迭代过程</strong></p>
<ul>
<li><p><code>data_iter</code>部分 <code>train_iter,test_iter</code></p>
<ul>
<li>加载数据<code>load_data(batch_size,num_steps,use_random_sample)</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># batch_size = 2</span><br><span class="hljs-comment"># 每次训练两个batch</span><br><span class="hljs-comment"># num_setp = 5</span><br><span class="hljs-comment"># 往后预测5个 train模式下，算5个的误差 重复两次(batch)</span><br><span class="hljs-comment"># X-&gt;y</span><br><span class="hljs-comment"># 3-&gt;5 5-&gt;13 13-&gt;2 2-&gt;1 1-&gt;13</span><br>tensor([[ <span class="hljs-number">3</span>,  <span class="hljs-number">5</span>, <span class="hljs-number">13</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">1</span>],<br>        [<span class="hljs-number">14</span>, <span class="hljs-number">11</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">1</span>, <span class="hljs-number">17</span>]])<br>tensor([[ <span class="hljs-number">5</span>, <span class="hljs-number">13</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">1</span>, <span class="hljs-number">13</span>],<br>        [<span class="hljs-number">11</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">1</span>, <span class="hljs-number">17</span>,  <span class="hljs-number">4</span>]])<br></code></pre></td></tr></table></figure></li>
<li><p><code>网络部分初始化</code></p>
<ul>
<li><span class="math inline">\(n\)</span>：<span
class="math inline">\(batch\_size\)</span>、<span
class="math inline">\(d\)</span>：<span
class="math inline">\(input\_dim\)</span>、<span
class="math inline">\(h\)</span>：<span
class="math inline">\(hidden\_num\)</span>
<ul>
<li>其中<span
class="math inline">\(d=h=vobal\_size=len(vobal)\)</span></li>
<li><span class="math inline">\(W\_xh\)</span>：<span
class="math inline">\(d\cdot h\)</span>, <span
class="math inline">\(W\_hh\)</span>：<span class="math inline">\(h\cdot
h\)</span>，<span class="math inline">\(b_h\)</span>：<span
class="math inline">\(1\cdot h\)</span>,（隐藏层参数）</li>
<li>$ W_hq<span class="math inline">\(：\)</span>hq<span
class="math inline">\(,\)</span> b_q$：<span
class="math inline">\(1\cdot q\)</span>（输出层参数）</li>
</ul></li>
<li><code>begin_state</code>:-&gt;<code>init_rnn_state</code>：隐藏层的隐藏单元的初始化
<span class="math inline">\(n\cdot h\)</span></li>
<li><code>get_params</code>:-&gt;<code>self.params</code>:隐藏层与输出层参数初始化<span
class="math inline">\([W_xh, W_hh, b_h, W_hq, b_q]\)</span></li>
</ul></li>
<li><p><code>网络部分计算</code></p>
<ul>
<li><p><span class="math inline">\(n · d · d · h\)</span> + $· h · q + 1
· q = n · q $ (<span class="math inline">\(x_{t-1}\)</span>计算<span
class="math inline">\(x_t\)</span>) + $· h · h + 1 · h = n · h
$（更新隐藏层）</p></li>
<li><p>具体过程</p>
<ul>
<li>onehot编码: input 5 · 2 --&gt; 5 · 2 · 32
<ul>
<li>num_step · batch_size --&gt; num_step · batch_size · vocal_size</li>
</ul></li>
</ul>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">F.one_hot(torch.tensor([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>]), <span class="hljs-number">5</span>) <br>tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>        [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])<br></code></pre></td></tr></table></figure></p>
<ul>
<li>forward计算</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">y = Y.T.reshape(-<span class="hljs-number">1</span>)<br>X, y = X.to(device), y.to(device)<br>y_hat, state = net(X, state)<br><span class="hljs-comment"># net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,init_rnn_state, rnn)</span><br><span class="hljs-comment"># rnn</span><br><span class="hljs-comment">#    for X in inputs:</span><br><span class="hljs-comment">#        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class="hljs-comment">#        Y = torch.mm(H, W_hq) + b_q</span><br><span class="hljs-comment">#        outputs.append(Y)</span><br></code></pre></td></tr></table></figure>
<ul>
<li>loss计算与反向传播</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">l = loss(y_hat, y.long()).mean()<br>updater.zero_grad()<br>l.backward()<br>grad_clipping(net, <span class="hljs-number">1</span>)<br>updater.step()<br></code></pre></td></tr></table></figure></li>
</ul></li>
</ul>
<h4 id="手动实现">手动实现</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-comment"># 加载数据 批量大小 32 时间步长 35</span><br>batch_size, num_steps = <span class="hljs-number">32</span>, <span class="hljs-number">35</span><br>train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 初始化隐藏层参数</span><br><span class="hljs-comment"># 28 512</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_params</span>(<span class="hljs-params">vocab_size, num_hiddens, device</span>):<br>    num_inputs = num_outputs = vocab_size<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">normal</span>(<span class="hljs-params">shape</span>):<br>        <span class="hljs-keyword">return</span> torch.randn(size=shape, device=device) * <span class="hljs-number">0.01</span><br><br>    <span class="hljs-comment"># 隐藏层参数</span><br>    <span class="hljs-comment"># 28 512 d x h</span><br>    W_xh = normal((num_inputs, num_hiddens))<br>    <span class="hljs-comment"># 512 512 h x h</span><br>    W_hh = normal((num_hiddens, num_hiddens))<br>    <span class="hljs-comment"># 512  1 x h</span><br>    b_h = torch.zeros(num_hiddens, device=device)<br>    <span class="hljs-comment"># 输出层参数</span><br>    <span class="hljs-comment"># 512 28 h x q(d)</span><br>    W_hq = normal((num_hiddens, num_outputs))<br>    <span class="hljs-comment"># 28 1 x q(d)</span><br>    b_q = torch.zeros(num_outputs, device=device)<br>    <span class="hljs-comment"># 附加梯度</span><br>    params = [W_xh, W_hh, b_h, W_hq, b_q]<br>    <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>        param.requires_grad_(<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> params<br> <br><span class="hljs-comment"># 初始化隐藏层状态· ht n x h</span><br><span class="hljs-comment"># 32 512</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_rnn_state</span>(<span class="hljs-params">batch_size, num_hiddens, device</span>):<br>  <span class="hljs-keyword">return</span> (torch.zeros((batch_size, num_hiddens)), device=device),)<br></code></pre></td></tr></table></figure>
<figure>
<img src="/img/RNN_init.jpg" srcset="/img/loading.gif" lazyload alt="隐藏层输入输出与参数" />
<figcaption aria-hidden="true">隐藏层输入输出与参数</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义RNN层</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">rnn</span>(<span class="hljs-params">inputs, state, params</span>):<br>    <span class="hljs-comment"># inputs的形状：(时间步数量，批量大小，词表大小)</span><br>    W_xh, W_hh, b_h, W_hq, b_q = params<br>    H, = state<br>    outputs = []<br>    <span class="hljs-comment"># X的形状：(批量大小，词表大小)</span><br>    <span class="hljs-keyword">for</span> X <span class="hljs-keyword">in</span> inputs:<br>        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)<br>        Y = torch.mm(H, W_hq) + b_q<br>        outputs.append(Y)<br>    <span class="hljs-keyword">return</span> torch.cat(outputs, dim=<span class="hljs-number">0</span>), (H,)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 封装</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RNNModelScratch</span>: <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, num_hiddens, device,</span><br><span class="hljs-params">                 get_params, init_state, forward_fn</span>):<br>        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens<br>        self.params = get_params(vocab_size, num_hiddens, device)<br>        self.init_state, self.forward_fn = init_state, forward_fn<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, X, state</span>):<br>        <span class="hljs-comment"># X 32 x 35 x 1  batch_size num_step seq</span><br>        X = F.one_hot(X.T, self.vocab_size).<span class="hljs-built_in">type</span>(torch.float32)<br>        <span class="hljs-comment"># X 35 x 32 x 28 num_step batch_size OnehotSeq</span><br>        <span class="hljs-keyword">return</span> self.forward_fn(X, state, self.params)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">begin_state</span>(<span class="hljs-params">self, batch_size, device</span>):<br>        <span class="hljs-keyword">return</span> self.init_state(batch_size, self.num_hiddens, device)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">num_hiddens = <span class="hljs-number">512</span><br>net = RNNModelScratch(<span class="hljs-built_in">len</span>(vocab), num_hiddens, d2l.try_gpu(), get_params,<br>                      init_rnn_state, rnn)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 预测</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_ch8</span>(<span class="hljs-params">prefix, num_preds, net, vocab, device</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;在prefix后面生成新字符&quot;&quot;&quot;</span><br>    state = net.begin_state(batch_size=<span class="hljs-number">1</span>, device=device)<br>    outputs = [vocab[prefix[<span class="hljs-number">0</span>]]]<br>    get_input = <span class="hljs-keyword">lambda</span>: torch.tensor([outputs[-<span class="hljs-number">1</span>]], device=device).reshape((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> prefix[<span class="hljs-number">1</span>:]:  <span class="hljs-comment"># 预热期</span><br>        _, state = net(get_input(), state)<br>        outputs.append(vocab[y])<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_preds):  <span class="hljs-comment"># 预测num_preds步</span><br>        y, state = net(get_input(), state)<br>        outputs.append(<span class="hljs-built_in">int</span>(y.argmax(dim=<span class="hljs-number">1</span>).reshape(<span class="hljs-number">1</span>)))<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> outputs])<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">grad_clipping</span>(<span class="hljs-params">net, theta</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;裁剪梯度&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        params = [p <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> net.parameters() <span class="hljs-keyword">if</span> p.requires_grad]<br>    <span class="hljs-keyword">else</span>:<br>        params = net.params<br>    norm = torch.sqrt(<span class="hljs-built_in">sum</span>(torch.<span class="hljs-built_in">sum</span>((p.grad ** <span class="hljs-number">2</span>)) <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> params))<br>    <span class="hljs-keyword">if</span> norm &gt; theta:<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>            param.grad[:] *= theta / norm<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_epoch_ch8</span>(<span class="hljs-params">net, train_iter, loss, updater, device, use_random_iter</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;训练网络一个迭代周期（定义见第8章）&quot;&quot;&quot;</span><br>    state, timer = <span class="hljs-literal">None</span>, d2l.Timer()<br>    metric = d2l.Accumulator(<span class="hljs-number">2</span>)  <span class="hljs-comment"># 训练损失之和,词元数量</span><br>    <span class="hljs-keyword">for</span> X, Y <span class="hljs-keyword">in</span> train_iter:<br>        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> use_random_iter:<br>            <span class="hljs-comment"># 在第一次迭代或使用随机抽样时初始化state</span><br>            state = net.begin_state(batch_size=X.shape[<span class="hljs-number">0</span>], device=device)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module) <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(state, <span class="hljs-built_in">tuple</span>):<br>                <span class="hljs-comment"># state对于nn.GRU是个张量</span><br>                state.detach_()<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># state对于nn.LSTM或对于我们从零开始实现的模型是个张量</span><br>                <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> state:<br>                    s.detach_()<br>        y = Y.T.reshape(-<span class="hljs-number">1</span>)<br>        X, y = X.to(device), y.to(device)<br>        y_hat, state = net(X, state)<br>        <span class="hljs-comment"># state 最后一步的状态，包含了前面所有信息的更新</span><br>        <span class="hljs-comment"># y: [1020] num_step x batch_size</span><br>        <span class="hljs-comment"># y_hat: [1020,28] num_step x batch_size x vocal_size</span><br>        <span class="hljs-comment"># loss 为CrossEntropy</span><br>        l = loss(y_hat, y.long()).mean()<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(updater, torch.optim.Optimizer):<br>            updater.zero_grad()<br>            l.backward()<br>            grad_clipping(net, <span class="hljs-number">1</span>)<br>            updater.step()<br>        <span class="hljs-keyword">else</span>:<br>            l.backward()<br>            grad_clipping(net, <span class="hljs-number">1</span>)<br>            <span class="hljs-comment"># 因为已经调用了mean函数</span><br>            updater(batch_size=<span class="hljs-number">1</span>)<br>        metric.add(l * y.numel(), y.numel())<br>    <span class="hljs-keyword">return</span> math.exp(metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">1</span>]), metric[<span class="hljs-number">1</span>] / timer.stop()<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_ch8</span>(<span class="hljs-params">net, train_iter, vocab, lr, num_epochs, device,</span><br><span class="hljs-params">              use_random_iter=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;训练模型（定义见第8章）&quot;&quot;&quot;</span><br>    loss = nn.CrossEntropyLoss()<br>    animator = d2l.Animator(xlabel=<span class="hljs-string">&#x27;epoch&#x27;</span>, ylabel=<span class="hljs-string">&#x27;perplexity&#x27;</span>,<br>                            legend=[<span class="hljs-string">&#x27;train&#x27;</span>], xlim=[<span class="hljs-number">10</span>, num_epochs])<br>    <span class="hljs-comment"># 初始化</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        updater = torch.optim.SGD(net.parameters(), lr)<br>    <span class="hljs-keyword">else</span>:<br>        updater = <span class="hljs-keyword">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size)<br>    predict = <span class="hljs-keyword">lambda</span> prefix: predict_ch8(prefix, <span class="hljs-number">50</span>, net, vocab, device)<br>    <span class="hljs-comment"># 训练和预测</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        ppl, speed = train_epoch_ch8(<br>            net, train_iter, loss, updater, device, use_random_iter)<br>        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(predict(<span class="hljs-string">&#x27;time traveller&#x27;</span>))<br>            animator.add(epoch + <span class="hljs-number">1</span>, [ppl])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;困惑度 <span class="hljs-subst">&#123;ppl:<span class="hljs-number">.1</span>f&#125;</span>, <span class="hljs-subst">&#123;speed:<span class="hljs-number">.1</span>f&#125;</span> 词元/秒 <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(device)&#125;</span>&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(predict(<span class="hljs-string">&#x27;time traveller&#x27;</span>))<br>    <span class="hljs-built_in">print</span>(predict(<span class="hljs-string">&#x27;traveller&#x27;</span>))<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 训练</span><br>num_epochs, lr = <span class="hljs-number">500</span>, <span class="hljs-number">1</span><br>train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())<br><span class="hljs-comment"># 调用网络预测</span><br>predict_ch8(<span class="hljs-string">&#x27;time traveller &#x27;</span>, <span class="hljs-number">10</span>, net, vocab, d2l.try_gpu())<br><br></code></pre></td></tr></table></figure>
<h4 id="简洁实现">简洁实现</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>batch_size, num_steps = <span class="hljs-number">32</span>, <span class="hljs-number">35</span><br>train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 初始化隐藏单元数量</span><br><span class="hljs-comment"># 初始化隐藏层</span><br>num_hiddens = <span class="hljs-number">256</span><br><span class="hljs-comment"># 利用nn.RNN()定义RNN层</span><br><span class="hljs-comment"># 没有输出层，需要在封装的网络类中加入nn.Linear()作为输出</span><br>rnn_layer = nn.RNN(<span class="hljs-built_in">len</span>(vocab), num_hiddens)<br><span class="hljs-comment"># 初始化隐藏层状态 （隐藏层数、批量大小、隐藏单元数）</span><br>state = torch.zeros((<span class="hljs-number">1</span>, batch_size, num_hiddens))<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RNNModel</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, rnn_layer, vocab_size, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(RNNModel, self).__init__(**kwargs)<br>        self.rnn = rnn_layer<br>        self.vocab_size = vocab_size<br>        self.num_hiddens = self.rnn.hidden_size<br>        <span class="hljs-comment"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.rnn.bidirectional:<br>            self.num_directions = <span class="hljs-number">1</span><br>            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)<br>        <span class="hljs-keyword">else</span>:<br>            self.num_directions = <span class="hljs-number">2</span><br>            self.linear = nn.Linear(self.num_hiddens * <span class="hljs-number">2</span>, self.vocab_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, state</span>):<br>        X = F.one_hot(inputs.T.long(), self.vocab_size)<br>        <span class="hljs-comment"># X 35 x 32 x 28 num_step batch_size OnehotSeq</span><br>        X = X.to(torch.float32)<br>        <span class="hljs-comment"># Y 35 x 32 x 256 num_step x batch_size x hidden_num</span><br>        <span class="hljs-comment"># state 1 x 32 x 256 最后一次训练的state state = Y[-1]</span><br>        Y, state = self.rnn(X, state)<br>        <span class="hljs-comment"># 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span><br>        <span class="hljs-comment"># 它的输出形状是(时间步数*批量大小,词表大小)。</span><br>        <span class="hljs-comment"># 每次的状态计算每次的输出计算loss</span><br>        output = self.linear(Y.reshape((-<span class="hljs-number">1</span>, Y.shape[-<span class="hljs-number">1</span>])))<br>        <span class="hljs-keyword">return</span> output, state<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">begin_state</span>(<span class="hljs-params">self, device, batch_size=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.rnn, nn.LSTM):<br>            <span class="hljs-comment"># nn.GRU以张量作为隐状态</span><br>            <span class="hljs-keyword">return</span>  torch.zeros((self.num_directions * self.rnn.num_layers,<br>                                 batch_size, self.num_hiddens),<br>                                device=device)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># nn.LSTM以元组作为隐状态</span><br>            <span class="hljs-keyword">return</span> (torch.zeros((<br>                self.num_directions * self.rnn.num_layers,<br>                batch_size, self.num_hiddens), device=device),<br>                    torch.zeros((<br>                        self.num_directions * self.rnn.num_layers,<br>                        batch_size, self.num_hiddens), device=device))<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">device = d2l.try_gpu()<br><span class="hljs-comment"># 定义网络</span><br>net = RNNModel(rnn_layer, vocab_size=<span class="hljs-built_in">len</span>(vocab))<br>net = net.to(device)<br>num_epochs, lr = <span class="hljs-number">500</span>, <span class="hljs-number">1</span><br><span class="hljs-comment"># 训练、预测</span><br>d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)<br>d2l.predict_ch8(<span class="hljs-string">&#x27;time traveller&#x27;</span>, <span class="hljs-number">10</span>, net, vocab, device)<br></code></pre></td></tr></table></figure>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Notes/" class="category-chain-item">Notes</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/DeepLearning/">#DeepLearning</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>深度学习 循环神经网络 笔记</div>
      <div>https://anonymouslosty.github.io/2023/07/05/深度学习 循环神经网络 笔记/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Ling yi</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年7月5日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2023年7月23日</div>
        </div>
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/07/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20%E7%AC%94%E8%AE%B0/" title="深度学习 现代循环神经网络 笔记">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">深度学习 现代循环神经网络 笔记</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/07/04/Kubernetes-%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%88%9D%E5%A7%8B%E5%8C%96/" title="Kubernetes 配置与初始化">
                        <span class="hidden-mobile">Kubernetes 配置与初始化</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":true,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
