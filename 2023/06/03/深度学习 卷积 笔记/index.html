

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/Github.png">
  <link rel="icon" href="/img/Github.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Ling yi">
  <meta name="keywords" content="">
  
    <meta name="description" content="教程 https:&#x2F;&#x2F;zh-v2.d2l.ai&#x2F;chapter_preface&#x2F;index.html">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习 卷积 笔记">
<meta property="og:url" content="https://anonymouslosty.github.io/2023/06/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E5%8D%B7%E7%A7%AF%20%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="anonymouslosty的Blog">
<meta property="og:description" content="教程 https:&#x2F;&#x2F;zh-v2.d2l.ai&#x2F;chapter_preface&#x2F;index.html">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://anonymouslosty.github.io/img/%E5%8D%B7%E7%A7%AFEG1.png">
<meta property="og:image" content="https://anonymouslosty.github.io/img/%E5%8D%B7%E7%A7%AF%E5%A1%AB%E5%85%85.png">
<meta property="og:image" content="https://anonymouslosty.github.io/img/%E5%8D%B7%E7%A7%AF%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93.png">
<meta property="og:image" content="https://anonymouslosty.github.io/img/1x1%E5%8D%B7%E7%A7%AF%E6%A0%B8%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="https://anonymouslosty.github.io/img/Pooling%E6%B1%A0%E5%8C%96.png">
<meta property="og:image" content="https://anonymouslosty.github.io/img/LeNet%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C.png">
<meta property="og:image" content="https://anonymouslosty.github.io/img/alexnet.svg">
<meta property="og:image" content="https://anonymouslosty.github.io/img/nn_Sequential.png">
<meta property="og:image" content="https://anonymouslosty.github.io/img/VGGvsNiN.png">
<meta property="og:image" content="https://anonymouslosty.github.io/img/googLeNet_Inception.png">
<meta property="og:image" content="https://anonymouslosty.github.io/img/inception-full.svg">
<meta property="og:image" content="https://anonymouslosty.github.io/img/resnet-block.svg">
<meta property="article:published_time" content="2023-06-03T05:10:44.000Z">
<meta property="article:modified_time" content="2023-06-15T07:27:27.617Z">
<meta property="article:author" content="Ling yi">
<meta property="article:tag" content="DeepLearning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://anonymouslosty.github.io/img/%E5%8D%B7%E7%A7%AFEG1.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>深度学习 卷积 笔记 - anonymouslosty的Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"anonymouslosty.github.io","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":90,"cursorChar":"..","loop":true,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4","placement":"left","visible":"hover","icon":"#"},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":true,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"left","headingSelector":"h1,h2,h3,h4","collapseDepth":2},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":true}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 80vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>anonymouslosty的Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/band_deer.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="深度学习 卷积 笔记"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Ling yi
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-06-03 13:10" pubdate>
          2023年6月3日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          23k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          195 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
        <div class="scroll-down-bar">
          <i class="iconfont icon-arrowdown"></i>
        </div>
      
    </div>
  </div>
</div>

</div>

    <a target="_blank" rel="noopener" href="https://github.com/anonymouslosty" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#fff; color:#151513; position: absolute; top: 0; border: 0; right: 0; z-index: 1031;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="padding-left: 2rem; margin-right: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">深度学习 卷积 笔记</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：2023年6月15日 下午
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <h2 id="一些理解">一些理解</h2>
<p>用神经网络去学习构建卷积核</p>
<p>卷积层将输入和核矩阵进行交叉相关，加上偏移之后得到输出。</p>
<p>核矩阵和偏移是可学习的参数</p>
<p>核矩阵的大小是超参数(在训练前就已经定义好)</p>
<p><strong>比较好的思想：高、宽减半，通道数翻一倍</strong></p>
<h2 id="二维交叉与二维卷积">二维交叉与二维卷积</h2>
<p>没有本质的区别，</p>
<p><span
class="math inline">\(y_{i,j}=\sum_{a=1}^{h}\sum_{b=1}^{w}w_{a,b}x_{i+a,j+b}\)</span></p>
<p><strong>索引前多了负号，因为<code>w</code>是学习的值，所以没有本质区别</strong></p>
<p><span
class="math inline">\(y_{i,j}=\sum_{a=1}^{h}\sum_{b=1}^{w}w_{-a,-b}x_{i+a,j+b}\)</span></p>
<h3 id="不同的维度">不同的维度</h3>
<ul>
<li><p>一维</p>
<p><span class="math inline">\(y_i=\sum_{a=1}^hw_ax_{i+a}\)</span></p>
<p>处理文本、语言、时间序列</p></li>
<li><p>三维</p>
<p><span
class="math inline">\(y_{i,j,k}=\sum_{a=1}^h\sum_{b=1}^w\sum_{c=1}^d\)</span></p></li>
</ul>
<h2 id="实现二维卷积层">实现二维卷积层</h2>
<h3 id="互相关运算">互相关运算</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">corr2d</span>(<span class="hljs-params">X,K</span>):<br>    h,w = K.shape<br>    Y = torch.zeros((X.shape[<span class="hljs-number">0</span>]-h+<span class="hljs-number">1</span>,X.shape[<span class="hljs-number">1</span>]-w+<span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">0</span>]):<br>    	<span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span> (Y.shape[<span class="hljs-number">1</span>]):<br>            Y[i,j]=(X[i:i+h,j:j+w]*K).<span class="hljs-built_in">sum</span>()<br>    <span class="hljs-keyword">return</span> Y<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Conv2D</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,kernel_size</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.weight = nn.Parameter(torch.rand(kernel_size))<br>        self.bias == nn.Parameter(torch.zeros(<span class="hljs-number">1</span>))<br>   	<span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,X</span>):<br>        <span class="hljs-keyword">return</span> corr2d(x,self.weight) + self.bias<br></code></pre></td></tr></table></figure>
<h3 id="例子">例子</h3>
<figure>
<img src="/img/卷积EG1.png" srcset="/img/loading.gif" lazyload alt="求卷积核说明图" />
<figcaption aria-hidden="true">求卷积核说明图</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br>conv2d = nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,kernel_size=(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),bias=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># 第一个张量矩阵</span><br>X = torch.ones((<span class="hljs-number">6</span>, <span class="hljs-number">8</span>))<br>X[:, <span class="hljs-number">2</span>:<span class="hljs-number">6</span>] = <span class="hljs-number">0</span><br>X = X.reshape((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,<span class="hljs-number">8</span>))<br><span class="hljs-comment"># 第二个张量矩阵</span><br>Y = torch.zeros((<span class="hljs-number">6</span>,<span class="hljs-number">7</span>))<br>Y[:,<span class="hljs-number">1</span>] = <span class="hljs-number">1</span><br>Y[:,-<span class="hljs-number">2</span>]= -<span class="hljs-number">1</span><br>Y = Y.reshape((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>))<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    Y_hat = conv2d(X)<br>    l = (Y_hat - Y) ** <span class="hljs-number">2</span><br>    conv2d.zero_grad()<br>    l.<span class="hljs-built_in">sum</span>().backward()<br>    <span class="hljs-comment"># 迭代卷积核</span><br>    conv2d.weight.data[:] -= <span class="hljs-number">3e-2</span> * conv2d.weight.grad<br>    <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;epoch <span class="hljs-subst">&#123;i+<span class="hljs-number">1</span>&#125;</span>, loss <span class="hljs-subst">&#123;l.<span class="hljs-built_in">sum</span>():<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(conv2d.weight.data.reshape((<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)))<br><span class="hljs-comment">#tensor([[ 1.0000, -1.0000]])</span><br><span class="hljs-comment">#表示 中间问号里的卷积核为[1,-1] 能够使得X卷积计算为Y</span><br></code></pre></td></tr></table></figure>
<h3 id="填充和步幅">填充和步幅</h3>
<h4 id="原因">原因</h4>
<p>每次使用卷积核都能减小输出的大小</p>
<ul>
<li><code>input</code> :<code>32x32</code>
<code>kernel_size</code>:<code>5x5</code>
<ul>
<li>根据<code>kernel_size</code> 每卷积一次，减少<code>4x4</code></li>
<li>第一次卷积 <code>32x32</code> :arrow_right:<code>28x28</code></li>
<li>...</li>
<li>第七次卷积结果 <code>4x4</code></li>
</ul></li>
<li>总结：每卷积一次，形状从<span class="math inline">\(n_h \times
n_w\)</span>减小到<span
class="math inline">\((n_w-k_h+1)\times(n_w-k_w+1)\)</span>
<ul>
<li>其中<code>k_h</code>为卷积核的高，<code>k_w</code>为卷积核的宽</li>
</ul></li>
</ul>
<h4 id="填充">填充</h4>
<figure>
<img src="/img/卷积填充.png" srcset="/img/loading.gif" lazyload alt="卷积填充" />
<figcaption aria-hidden="true">卷积填充</figcaption>
</figure>
<p><strong>在输入数据的四周进行数据填充</strong>。</p>
<ul>
<li>在上下左右分别添加0
<ul>
<li>输出形状为<span class="math inline">\((n_h-k_h+p_h+1)
\times(n_w-k_w+p_w+1)\)</span></li>
<li>通常取 <span class="math inline">\(p_h=k_h-1\)</span>,<span
class="math inline">\(p_w=k_w-1\)</span>
<ul>
<li><code>k</code>为奇数，在上下两侧填充<span
class="math inline">\(p_h/2\)</span></li>
<li><code>k</code>为偶数，在上侧填充<span
class="math inline">\([p_h/2]\)</span>，在下侧填充<span
class="math inline">\([p_h/2]\)</span></li>
</ul></li>
</ul></li>
</ul>
<h4 id="步幅">步幅</h4>
<p><strong>控制卷积核移动的幅度，可以每次不止移动一格。</strong></p>
<p>可以使得在输入较大、卷积核较小的情况下，快速达到较小的输出，减少中间的卷积层数。</p>
<ul>
<li>给定高度<span class="math inline">\(s_h\)</span>和宽度<span
class="math inline">\(s_w\)</span>的步幅，输出形状为：<span
class="math inline">\([(n_h-k_h+p_h+s_h)/s_h]\times[(n_w-k_w+p_w+s_w)/s_w]\)</span>
<ul>
<li>如果 <span class="math inline">\(p_h=k_h-1\)</span>,<span
class="math inline">\(p_w=k_w-1\)</span>，输出形状为：<span
class="math inline">\([(n_h+s_h-1)/s_h]\times[(n_w+s_w-1)/s_w]\)</span></li>
<li>如果输入高度和宽度可以被步幅整除，输出形状为：<span
class="math inline">\((n_h/s_h)\times(n_w/s_w)\)</span></li>
</ul></li>
</ul>
<h4 id="总结">总结</h4>
<ul>
<li>填充和步幅都是卷积层的超参数，声明网络的时候加上就行</li>
<li>填充在输入周围添加额外的行/列，来控制输出形状的减少量，常设为<code>p = k-1</code>核-1</li>
<li>步幅是每次滑动核窗口时的行/列的步长，可以成倍地减少输出形状</li>
</ul>
<h4 id="例子-1">例子</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-comment"># 在所有侧边填充一个像素</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">comp_conv2d</span>(<span class="hljs-params">conv2d,X</span>):<br>    <span class="hljs-comment"># （批量大小、通道、高度、宽度）</span><br>    X = X.reshape((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)+X.shape)<br><br>    Y = conv2d(X)<br>    <span class="hljs-keyword">return</span> Y.reshape(Y.shape[<span class="hljs-number">2</span>:])<br><br><span class="hljs-comment"># 定义一个输入输出通道数为1，kernel大小为3，填充为1的卷积神经网络</span><br>conv2d = nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>X = torch.rand(size=(<span class="hljs-number">8</span>,<span class="hljs-number">8</span>))<br><span class="hljs-built_in">print</span>(comp_conv2d(conv2d,X).shape)<br><span class="hljs-comment"># 8 x 8</span><br><br><span class="hljs-comment"># kernal 宽高不一样的时候，注意padding填充的设置</span><br><span class="hljs-comment"># 8-5+ 2x2(上下填充的)+1 = 8</span><br><span class="hljs-comment"># 8-3+ 1x2(左右填充的)+1 = 8</span><br><span class="hljs-comment"># (kernel_size - 1 )/ 2</span><br>conv2d = nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,kernel_size=(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>),padding=(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>))<br><span class="hljs-built_in">print</span>(comp_conv2d(conv2d,X).shape)<br><span class="hljs-comment"># 8 x 8</span><br><br><span class="hljs-comment"># 步幅为2的情况</span><br>conv2d = nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>,stride=<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(comp_conv2d(conv2d,X).shape)<br><span class="hljs-comment"># 4 x 4</span><br><br>conv2d = nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,kernel_size=(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>),padding=(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>),stride=(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>))<br><span class="hljs-built_in">print</span>(comp_conv2d(conv2d,X).shape)<br><span class="hljs-comment"># 2 x 2 </span><br><span class="hljs-comment"># [(8-3+0x2+3)/3] = 2</span><br><span class="hljs-comment"># [(8-5+1x2+4)/4] = 2</span><br></code></pre></td></tr></table></figure>
<h3 id="输入和输出通道">输入和输出通道</h3>
<figure>
<img src="/img/卷积输入通道.png" srcset="/img/loading.gif" lazyload alt="多输入通道的计算方式" />
<figcaption aria-hidden="true">多输入通道的计算方式</figcaption>
</figure>
<p><strong>每个通道有自己的卷积核</strong></p>
<p><span class="math inline">\(c_i\)</span>:输入通道
<code>channel_input</code></p>
<p><span
class="math inline">\(c_o\)</span>:输出通道<code>channel_output</code></p>
<p><strong>单输入通道</strong></p>
<p>每个通道和自己的卷积核进行计算</p>
<ul>
<li>输入 <code>X</code>:<span class="math inline">\(c_i\times n_h\times
n_w\)</span></li>
<li>核<code>W</code>:<span class="math inline">\(c_i\times k_h\times
k_w\)</span></li>
<li>输出<code>Y</code>:<span class="math inline">\(m_h\times
m_w\)</span></li>
</ul>
<p><strong>多输入通道</strong></p>
<p><strong>多个卷积核、每个核生成一个通道</strong></p>
<p><strong>每个通道可有用于识别不同的特定模式</strong></p>
<ul>
<li>输入 <code>X</code>:<span class="math inline">\(c_i\times n_h\times
n_w\)</span></li>
<li>核<code>W</code>:<span class="math inline">\(c_o\times c_i\times
k_h\times k_w\)</span></li>
<li>输出<code>Y</code>:<span class="math inline">\(c_o\times m_h\times
m_w\)</span></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#多输入通道</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">corr2d_multi_in</span>(<span class="hljs-params">X,K</span>):<br>    <span class="hljs-comment"># zip 将每一个输入和对应的卷积核绑定</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(d2l.corr2d(x,k) <span class="hljs-keyword">for</span> x,k <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(X,K))<br><br><br>X = torch.tensor([[[<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>], [<span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">5.0</span>], [<span class="hljs-number">6.0</span>, <span class="hljs-number">7.0</span>, <span class="hljs-number">8.0</span>]],<br>               [[<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>], [<span class="hljs-number">4.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">6.0</span>], [<span class="hljs-number">7.0</span>, <span class="hljs-number">8.0</span>, <span class="hljs-number">9.0</span>]]])<br>K = torch.tensor([[[<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>], [<span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>]], [[<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>], [<span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>]]])<br><br><span class="hljs-built_in">print</span>(corr2d_multi_in(X, K))<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#多输出通道</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">corr2d_multi_in_out</span>(<span class="hljs-params">X,K</span>):<br>    <span class="hljs-keyword">return</span> torch.stack([corr2d_multi_in(X,k) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> K],<span class="hljs-number">0</span>)<br><br>K = torch.reshape((K,K+<span class="hljs-number">1</span>,K+<span class="hljs-number">2</span>),<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(K.shape)<br></code></pre></td></tr></table></figure>
<h4 id="x1卷积层"><code>1x1</code>卷积层</h4>
<p>参考：<a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&amp;mid=2247602164&amp;idx=5&amp;sn=42c533969680bbf1ce39c058304cd2bd&amp;chksm=fb54ad18cc23240e77d18d91b3206ef1bde924f2f4a711516701c5ec5fe7511207b6a534b041&amp;scene=27">一文读懂卷积神经网络中的1x1卷积核
(qq.com)</a></p>
<p>不识别空间模式(没有周边像元参与)，只是<strong>对多个通道同一位置的像元进行融合</strong></p>
<p><span class="math inline">\(k_h=k_w=1\)</span></p>
<figure>
<img src="/img/1x1卷积核计算过程.png" srcset="/img/loading.gif" lazyload alt="1x1卷积核计算过程" />
<figcaption aria-hidden="true">1x1卷积核计算过程</figcaption>
</figure>
<p>相当于输入形状为<span class="math inline">\(n_hn_w\times
c_i\)</span>，权重为<span class="math inline">\(c_o\times
c_i\)</span>的全连接层</p>
<ul>
<li><span class="math inline">\(z=W\cdot x^T+b\)</span>
<ul>
<li><span class="math inline">\(W: \space c_o \times c_i\)</span></li>
<li><span class="math inline">\(x:\space n_hn_w \times c_i\)</span></li>
</ul></li>
<li>对应位置按权重(<code>Kernel</code>)相加</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">corr2d_multi_in_out_1x1</span>(<span class="hljs-params">X,K</span>):<br>    <span class="hljs-comment"># 数据的维度、长宽</span><br>    c_i,h,w = X.shape<br>    <span class="hljs-comment"># 核的个数，数据的输出维度</span><br>    c_o = K.shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-comment"># X 拉成 h*w x c_i的张量</span><br>    X = X.reshape((c_i,h*w))<br>    <span class="hljs-comment"># 卷积核，变成权重W</span><br>    K= K.reshape((c_o,c_i))<br>    <span class="hljs-comment"># 实现全连接层的操作</span><br>    Y = torch.matmul(K,X)<br>    <span class="hljs-comment"># 还原成矩阵</span><br>    <span class="hljs-keyword">return</span> Y.reshape((c_o,h,w))<br><br><span class="hljs-comment"># 输入</span><br>X = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br><span class="hljs-comment"># 卷积层 输出2 输入3 大小 1x1</span><br>K = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br><br>Y1 = corr2d_multi_in_out_1x1(X, K)<br>Y2 = corr2d_multi_in_out(X, K)<br><span class="hljs-keyword">assert</span> <span class="hljs-built_in">float</span>(torch.<span class="hljs-built_in">abs</span>(Y1 - Y2).<span class="hljs-built_in">sum</span>()) &lt; <span class="hljs-number">1e-6</span><br></code></pre></td></tr></table></figure>
<h4 id="总结-1">总结</h4>
<ul>
<li>输出通道数是卷积层的超参数</li>
<li>每个输入通道有独立的二维卷积核，所有通道结果相加得到一个输出通道结果</li>
<li>每个输出通道有独立的三维卷积核</li>
<li><code>[3,2,2,2]</code>
输出通道为3，输入通道为2，卷积核高2，卷积核宽2
<ul>
<li>有三个卷积核，每个卷积核有两层(是二维)，卷积核大小为2x2</li>
</ul></li>
</ul>
<h3 id="池化">池化</h3>
<h4 id="二维最大池化">二维最大池化</h4>
<figure>
<img src="/img/Pooling池化.png" srcset="/img/loading.gif" lazyload alt="Pooling池化" />
<figcaption aria-hidden="true">Pooling池化</figcaption>
</figure>
<p>返回滑动窗口中的最大值</p>
<ul>
<li>有填充和步幅</li>
<li>没有可学习的参数，只有取最大的操作子</li>
<li>每个输入通道应用池化层获得相应的输出通道</li>
<li>输入通道数=输出通道数</li>
</ul>
<h4 id="二维平均池化">二维平均池化</h4>
<ul>
<li>和最大池化一样，操作子变为取平均</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 手写版本</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pool2d</span>(<span class="hljs-params">X,pool_size,mode=<span class="hljs-string">&#x27;max&#x27;</span></span>):<br>    p_h,p_w = pool_size<br>    Y = torch.zeros((X.shape[<span class="hljs-number">0</span>]-p_h+<span class="hljs-number">1</span>,X.shape[<span class="hljs-number">1</span>]-p_w+<span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">0</span>]):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Y.shape[<span class="hljs-number">1</span>]):<br>            <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;max&#x27;</span>:<br>                Y[i,j] = X[i:i+p_h,j:j+p_w].<span class="hljs-built_in">max</span>()<br>            <span class="hljs-keyword">elif</span> mode ==<span class="hljs-string">&#x27;avg&#x27;</span>:<br>                Y[i,j] = X[i:i+p_h,j:j+p_w].mean()<br>             <br>     <span class="hljs-keyword">return</span> Y<br><br><span class="hljs-comment"># 创建最大池化层，窗口大小为3x3</span><br><span class="hljs-comment"># 框架默认步幅与池化窗口大小相同，所以步幅也为3</span><br>pool2d = nn.MaxPool2d(<span class="hljs-number">3</span>)<br><span class="hljs-comment"># 也可以手动指定padding和stride</span><br>pool2d = nn.MaxPool2d(<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>,stride=<span class="hljs-number">2</span>)<br><span class="hljs-comment"># 窗口大小也可以自己指定</span><br>pool2d = nn.MaxPool2d((<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),padding=(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),stride=(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>))<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#池化层多个通道中，每个通道单独计算</span><br><span class="hljs-comment"># 2 x 4 x 4</span><br>X = torch.cat((X,X+<span class="hljs-number">1</span>),<span class="hljs-number">1</span>)<br><br>pool2d = nn.MaxPool2d(<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>,stride=<span class="hljs-number">2</span>)<br><span class="hljs-comment"># 2 x 2 x 2</span><br>pool2d(X)<br><br></code></pre></td></tr></table></figure>
<h4 id="总结-2">总结</h4>
<ul>
<li>池化层返回窗口中的最大或者平均值，取决于操作子</li>
<li>池化层能缓解卷积层对位置的敏感性</li>
<li>同样有窗口大小、填充和步幅作为超参数</li>
</ul>
<h3 id="卷积层全连接层参数个数计算">卷积层、全连接层参数个数计算</h3>
<p>参考：<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/77471991">CNN卷积层、全连接层的参数量、计算量</a></p>
<ul>
<li>卷积层参数量：卷积核元素的大小
<ul>
<li>计算公式：<strong>参数量=（filter size * 前一层特征图的通道数 ）*
当前层filter数量</strong></li>
</ul></li>
<li>全连接层参数量：
<ul>
<li>计算公式：见参考</li>
</ul></li>
</ul>
<h2 id="批量归一化">批量归一化</h2>
<h3 id="问题">问题</h3>
<p>底部：靠近数据</p>
<p>顶部：靠近损失</p>
<p>训练过程中，方差和均值的分布在不同层之间发生变化</p>
<p>在持续的训练过程中，上面的收敛比较快，下面变化比较慢。</p>
<p>但是每次底部变化，底层的信息变了，顶部需要进行重新训练。</p>
<p>这个问题会导致收敛比较慢。</p>
<h3 id="原因-1">原因</h3>
<ul>
<li>损失在网络计算的最后(上部)，网络后面的层的训练比较快</li>
<li>数据在网络的最底部
<ul>
<li>底层训练很慢</li>
<li>底层变化，整个网络的数据都发生变化</li>
<li>最后的那些层需要学习很多次</li>
<li>导致收敛变慢</li>
</ul></li>
</ul>
<h3 id="解决方法">解决方法</h3>
<p>把不同层之间的均值和方差的分布固定住。</p>
<p>固定小批量里的均值和方差</p>
<p><span class="math inline">\(\begin{split}\begin{aligned}
\hat{\boldsymbol{\mu}}_\mathcal{B} &amp;= \frac{1}{|\mathcal{B}|}
\sum_{\mathbf{x} \in \mathcal{B}} \mathbf{x},\\
\hat{\boldsymbol{\sigma}}_\mathcal{B}^2 &amp;= \frac{1}{|\mathcal{B}|}
\sum_{\mathbf{x} \in \mathcal{B}} (\mathbf{x} -
\hat{\boldsymbol{\mu}}_{\mathcal{B}})^2 +
\epsilon.\end{aligned}\end{split}\)</span></p>
<p><span class="math inline">\(\mathrm{BN}(\mathbf{x}) =
\boldsymbol{\gamma} \odot \frac{\mathbf{x} -
\hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}}
+ \boldsymbol{\beta}.\)</span></p>
<p><span class="math inline">\(\gamma \quad \beta\)</span>
可学习的参数，当变为标准正态分布可能不太合适的话，可以用数据重新学习一个合适的方差和均值对数据进行优化。</p>
<h3 id="批量归一化层">批量归一化层</h3>
<ul>
<li>可学习的参数为<span
class="math inline">\(\gamma\quad\beta\)</span></li>
<li>作用在
<ul>
<li>全连接层和卷积层输入上</li>
<li>全连接层和卷积层输出上，激活函数之前
<ul>
<li>对输出数据减去均值除以方差，加上可以学习的<span
class="math inline">\(\gamma\quad\beta\)</span>，在加上激活函数</li>
</ul></li>
</ul></li>
<li>对于全连接层，作用在特征维
<ul>
<li>全连接层输入是二维的</li>
<li>行是样本，列是特征。</li>
<li>作用在特征维度是对特征求均值和方差（列）</li>
</ul></li>
<li>对于卷积层，作用在通道维
<ul>
<li>二维拓展到三维</li>
<li><code>Batch size x Channels x Height x Width</code>
<ul>
<li>样本数量： <code>Batch size x Height x Width</code>个像素</li>
<li>每个像素：<code>Channels</code> 个通道(特征)</li>
</ul></li>
<li>对每个像素的通道计算均值和方差</li>
</ul></li>
</ul>
<h3 id="代码">代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-comment"># moving_mean,moving_var 整个数据集的均值和方差</span><br><span class="hljs-comment"># eps 极小值 避免数据集中含有0</span><br><span class="hljs-comment"># momentum 更新moving_mean，moving_var</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">batch_norm</span>(<span class="hljs-params">X,gamma,beta,moving_mean,moving_var,eps,momentum</span>):<br>    <span class="hljs-comment"># 不算梯度，在做Inferance 推理</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.is_grad_enabled():<br>        <span class="hljs-comment"># 为什么用全局的均值和方差</span><br>        <span class="hljs-comment"># 做推理的时候很多时候输入只是一个样本，一般算不出自己的均值和方差</span><br>        X_hat = (X-moving_mean)/torch.sqrt(moving_var+eps)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># 限定输入类型为全连接层和卷积层</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(X.shape) <span class="hljs-keyword">in</span> (<span class="hljs-number">2</span>,<span class="hljs-number">4</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(X.shape)==<span class="hljs-number">2</span>:<br>            <span class="hljs-comment"># 批量大小、特征维度</span><br>            <span class="hljs-comment"># 对每一列的所有（行数个）样本求平均</span><br>            <span class="hljs-comment"># dim=0 压缩行维度</span><br>            <span class="hljs-comment"># 最后得到一个行向量</span><br>            mean = X.mean(dim=<span class="hljs-number">0</span>)<br>            var = ((X-mean)**<span class="hljs-number">2</span>).mean(dim=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 批量大小 通道数 高 宽 0 1 2 3</span><br>            <span class="hljs-comment"># dim=(0,2,3) 把批量大小、高、宽的全部通道数求均值</span><br>            <span class="hljs-comment"># keepdim = True 最终结果 1 x n x 1 x 1</span><br>            mean = X.mean(dim=(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),keepdim=<span class="hljs-literal">True</span>)<br>            var = ((X-mean)**<span class="hljs-number">2</span>).mean(dim=(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),keepdim=<span class="hljs-literal">True</span>)<br>        X_hat = (X-mean) / torch.sqrt(var+eps)<br>        moving_mean = momentum * moving_mean +(<span class="hljs-number">1.0</span>-momentum)*mean<br>        moving_var = momentum * moving_var + (<span class="hljs-number">1.0</span>-momentum)*var<br>    Y = gamma * X_hat +beta<br>    <br>    <span class="hljs-keyword">return</span> Y,moving_mean.data,moving_var.data<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BatchNorm</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,num_features,num_dims</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-keyword">if</span> num_dims==<span class="hljs-number">2</span>:<br>            shape = (<span class="hljs-number">1</span>,num_features)<br>        <span class="hljs-keyword">else</span>:<br>            shape=(<span class="hljs-number">1</span>,num_features,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># nn.Parameter() 将张量注册为模块的参数</span><br>        self.gamma = nn.Parameter(torch.ones(shape))<br>        self.beta  = nn.Parameter(torch.zeros(shape))<br>        self.moving_mean = torch.zeros(shape)<br>        self.moving_var = torch.ones(shape)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,X</span>):<br>        <span class="hljs-keyword">if</span> self.moving_mean.device != X.device:<br>            self.moving_mean = self.moving_mean.to(X.device)<br>            self.moving_var = self.moving_var.to(X.device)<br>        <br>        Y,self.moving_mean,self.moving_var = batch_norm(X,self.gamma,self.beta,self.moving_mean,self.moving_var,eps=<span class="hljs-number">1e-5</span>,momentum=<span class="hljs-number">0.9</span>)<br>        <br>        <span class="hljs-keyword">return</span> Y<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">net = nn.Sequential(<br>    <span class="hljs-comment"># 输入通道1 输出通道6 卷积核大小 5 </span><br>    nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,kernel_size=<span class="hljs-number">5</span>),BatchNorm(<span class="hljs-number">6</span>,num_dims=<span class="hljs-number">4</span>),nn.Sigmoid(),nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>),<br>    nn.Conv2d(<span class="hljs-number">6</span>,<span class="hljs-number">16</span>,kernel_size=<span class="hljs-number">5</span>),BatchNorm(<span class="hljs-number">16</span>,num_dims=<span class="hljs-number">4</span>),nn.Sigmoid(),nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>),<br>    nn.Flatten(),nn.Linear(<span class="hljs-number">16</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>,<span class="hljs-number">120</span>),<br>    BatchNorm(<span class="hljs-number">120</span>,num_dims=<span class="hljs-number">2</span>),nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>),BatchNorm(<span class="hljs-number">84</span>,num_dims=<span class="hljs-number">2</span>),<br>    nn.Sigmoid(),nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>)<br>)<br><br><span class="hljs-comment">#简明版本 调用nn.BatchNorm2d()</span><br>net = nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">5</span>), nn.BatchNorm2d(<span class="hljs-number">6</span>), nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>), nn.BatchNorm2d(<span class="hljs-number">16</span>), nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>), nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">120</span>), nn.BatchNorm1d(<span class="hljs-number">120</span>), nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>), nn.BatchNorm1d(<span class="hljs-number">84</span>), nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs, batch_size = <span class="hljs-number">1.0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">256</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure>
<h3 id="总结-3">总结</h3>
<ul>
<li>批量归一化固定小批量中的均值和方差，然后学习出合适的偏移和缩放</li>
<li>可以加速收敛速度，但是一般不改变模型精度</li>
<li>可以允许用更大的学习率进行训练</li>
</ul>
<h2 id="lenet-经典卷积神经网络">LeNet 经典卷积神经网络</h2>
<figure>
<img src="/img/LeNet卷积网络.png" srcset="/img/loading.gif" lazyload alt="LeNet卷积网络" />
<figcaption aria-hidden="true">LeNet卷积网络</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Reshape</span>(torch.nn.Moudle):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        <span class="hljs-comment"># view()相当于reshape、resize，对Tensor的形状进行调整。</span><br>        <span class="hljs-keyword">return</span> x.view(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)<br>    <br>net = nn.Sequential(<br>    Reshape(),<br>    <span class="hljs-comment"># 输入为1 输出为6 卷积核 5x5 padding=2 28x28 --&gt; 32x32</span><br>    <span class="hljs-comment"># nn.Sigmoid()函数的作用？还不是很清楚</span><br>    nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,kernel_size=<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>),<br>    nn.Conv2d(<span class="hljs-number">6</span>,<span class="hljs-number">16</span>,kernel_size=<span class="hljs-number">5</span>),nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>),<br>    <span class="hljs-comment"># 卷积层输出的是4D数据 1 x 16 x 5 x 5 1:批量</span><br>    <span class="hljs-comment"># 拉平为一维数据输入多层感知机</span><br>    nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>,<span class="hljs-number">120</span>),nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>),nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>)<br>)<br><br><span class="hljs-comment"># Reshape 批量：1 输出通道：1 高、宽 28</span><br>Reshape output shape: 	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br><span class="hljs-comment"># Padding之后：高、宽 32 批量：1 输出通道：6 核大小5 32-5+1 = 28 输出的高、宽还是28</span><br>Conv2d output shape: 	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br><span class="hljs-comment"># 激活函数 对shape不做操作</span><br>Sigmoid output shape: 	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br><span class="hljs-comment"># 平均池化层 核大小2 步长2</span><br>AvgPool2d output shape: 	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>])<br><span class="hljs-comment"># 批量：1 输出通道：16 输出高、宽：10 (16个核 每个核对6个通道加权求和再输出)</span><br>Conv2d output shape: 	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>])<br><span class="hljs-comment"># 激活函数 对shape不做操作</span><br>Sigmoid output shape: 	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>])<br><span class="hljs-comment"># 批量：1 输出通道：16 输出高、宽：5</span><br>AvgPool2d output shape: 	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>])<br><span class="hljs-comment"># 批量：1 拉平操作 将 1 x 16 x 5 x 5 的 4D 数据降低为一个维度</span><br>Flatten output shape: 	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">400</span>])<br><span class="hljs-comment"># MLP 隐藏层 400-&gt;120</span><br>Linear output shape: 	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">120</span>])<br><span class="hljs-comment"># 激活函数 对shape不做操作</span><br>Sigmoid output shape: 	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">120</span>])<br><span class="hljs-comment"># MLP 隐藏层 120-&gt;84</span><br>Linear output shape: 	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">84</span>])<br><span class="hljs-comment"># 激活函数 对shape不做操作</span><br>Sigmoid output shape: 	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">84</span>])<br><span class="hljs-comment"># MLP  84-&gt;10</span><br>Linear output shape: 	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">10</span>])<br><br><br>batch_size = <span class="hljs-number">256</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_accuracy_gpu</span>(<span class="hljs-params">net, data_iter, device=<span class="hljs-literal">None</span></span>): <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        net.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 设置为评估模式</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> device:<br>            <span class="hljs-comment"># 设置训练的device</span><br>            device = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(net.parameters())).device<br>    <span class="hljs-comment"># 正确预测的数量，总预测的数量</span><br>    metric = d2l.Accumulator(<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> data_iter:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(X, <span class="hljs-built_in">list</span>):<br>                <span class="hljs-comment"># BERT微调所需的（之后将介绍）</span><br>                X = [x.to(device) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X]<br>            <span class="hljs-keyword">else</span>:<br>                X = X.to(device)<br>            y = y.to(device)<br>            metric.add(d2l.accuracy(net(X), y), y.numel())<br>    <span class="hljs-keyword">return</span> metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">1</span>]<br><br><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_ch6</span>(<span class="hljs-params">net, train_iter, test_iter, num_epochs, lr, device</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">m</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.Linear <span class="hljs-keyword">or</span> <span class="hljs-built_in">type</span>(m) == nn.Conv2d:<br>            <span class="hljs-comment"># 全连接层和卷积层用xavier_uniform初始化</span><br>            <span class="hljs-comment"># 根据输入输出大小，使得用随机数的时候，输入和输出的方差是差不多的</span><br>            nn.init.xavier_uniform_(m.weight)<br>    net.apply(init_weights)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;training on&#x27;</span>, device)<br>    net.to(device)<br>    <span class="hljs-comment"># SGD优化</span><br>    optimizer = torch.optim.SGD(net.parameters(), lr=lr)<br>    <span class="hljs-comment"># 分类用</span><br>    loss = nn.CrossEntropyLoss()<br>    animator = d2l.Animator(xlabel=<span class="hljs-string">&#x27;epoch&#x27;</span>, xlim=[<span class="hljs-number">1</span>, num_epochs],<br>                            legend=[<span class="hljs-string">&#x27;train loss&#x27;</span>, <span class="hljs-string">&#x27;train acc&#x27;</span>, <span class="hljs-string">&#x27;test acc&#x27;</span>])<br>    timer, num_batches = d2l.Timer(), <span class="hljs-built_in">len</span>(train_iter)<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        <span class="hljs-comment"># 训练损失之和，训练准确率之和，样本数</span><br>        metric = d2l.Accumulator(<span class="hljs-number">3</span>)<br>        net.train()<br>        <span class="hljs-keyword">for</span> i, (X, y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_iter):<br>            timer.start()<br>            optimizer.zero_grad()<br>            X, y = X.to(device), y.to(device)<br>            y_hat = net(X)<br>            l = loss(y_hat, y)<br>            l.backward()<br>            optimizer.step()<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                metric.add(l * X.shape[<span class="hljs-number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="hljs-number">0</span>])<br>            timer.stop()<br>            train_l = metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">2</span>]<br>            train_acc = metric[<span class="hljs-number">1</span>] / metric[<span class="hljs-number">2</span>]<br>            <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % (num_batches // <span class="hljs-number">5</span>) == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> i == num_batches - <span class="hljs-number">1</span>:<br>                animator.add(epoch + (i + <span class="hljs-number">1</span>) / num_batches,<br>                             (train_l, train_acc, <span class="hljs-literal">None</span>))<br>        test_acc = evaluate_accuracy_gpu(net, test_iter)<br>        animator.add(epoch + <span class="hljs-number">1</span>, (<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, test_acc))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;loss <span class="hljs-subst">&#123;train_l:<span class="hljs-number">.3</span>f&#125;</span>, train acc <span class="hljs-subst">&#123;train_acc:<span class="hljs-number">.3</span>f&#125;</span>, &#x27;</span><br>          <span class="hljs-string">f&#x27;test acc <span class="hljs-subst">&#123;test_acc:<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;metric[<span class="hljs-number">2</span>] * num_epochs / timer.<span class="hljs-built_in">sum</span>():<span class="hljs-number">.1</span>f&#125;</span> examples/sec &#x27;</span><br>          <span class="hljs-string">f&#x27;on <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(device)&#125;</span>&#x27;</span>)<br><br>lr, num_epochs = <span class="hljs-number">0.9</span>, <span class="hljs-number">10</span><br>train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure>
<h2 id="alexnet-卷积神经网络">AlexNet 卷积神经网络</h2>
<h3 id="特点">特点</h3>
<p><strong>更大更深的<code>LeNet</code></strong></p>
<figure>
<img src="/img/alexnet.svg" srcset="/img/loading.gif" lazyload alt="AlexNet与LeNet对比" />
<figcaption aria-hidden="true">AlexNet与LeNet对比</figcaption>
</figure>
<h3 id="主要改进"><strong>主要改进</strong></h3>
<ul>
<li>丢弃法</li>
<li><code>ReLU</code></li>
<li><code>MaxPooling</code></li>
</ul>
<h3 id="代码-1">代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>net = nn.Sequential(<br>    <span class="hljs-comment"># 这里使用一个11*11的更大窗口来捕捉对象。</span><br>    <span class="hljs-comment"># 同时，步幅为4，以减少输出的高度和宽度。</span><br>    <span class="hljs-comment"># 另外，输出通道的数目远大于LeNet</span><br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">96</span>, kernel_size=<span class="hljs-number">11</span>, stride=<span class="hljs-number">4</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    <span class="hljs-comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span><br>    nn.Conv2d(<span class="hljs-number">96</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    <span class="hljs-comment"># 使用三个连续的卷积层和较小的卷积窗口。</span><br>    <span class="hljs-comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span><br>    <span class="hljs-comment"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span><br>    nn.Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.Conv2d(<span class="hljs-number">384</span>, <span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.Conv2d(<span class="hljs-number">384</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Flatten(),<br>    <span class="hljs-comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span><br>    nn.Linear(<span class="hljs-number">6400</span>, <span class="hljs-number">4096</span>), nn.ReLU(),<br>    nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>    nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>), nn.ReLU(),<br>    nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>    <span class="hljs-comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span><br>    nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 查看数据再Net中如何变换</span><br>X = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X=layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">&#x27;output shape:\t&#x27;</span>,X.shape)<br><br><span class="hljs-comment">#nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1)</span><br>Conv2d output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">96</span>, <span class="hljs-number">54</span>, <span class="hljs-number">54</span>])<br>ReLU output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">96</span>, <span class="hljs-number">54</span>, <span class="hljs-number">54</span>])<br><span class="hljs-comment"># nn.MaxPool2d(kernel_size=3, stride=2)</span><br>MaxPool2d output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">96</span>, <span class="hljs-number">26</span>, <span class="hljs-number">26</span>])<br><span class="hljs-comment"># nn.Conv2d(96, 256, kernel_size=5, padding=2)</span><br>Conv2d output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">26</span>, <span class="hljs-number">26</span>])<br>ReLU output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">26</span>, <span class="hljs-number">26</span>])<br><span class="hljs-comment"># nn.MaxPool2d(kernel_size=3, stride=2)</span><br>MaxPool2d output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])\<br><span class="hljs-comment"># nn.Conv2d(256, 384, kernel_size=3, padding=1)</span><br>Conv2d output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">384</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br>ReLU output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">384</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br><span class="hljs-comment"># nn.Conv2d(384, 384, kernel_size=3, padding=1)</span><br>Conv2d output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">384</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br>ReLU output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">384</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br><span class="hljs-comment"># nn.Conv2d(384, 256, kernel_size=3, padding=1)</span><br>Conv2d output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br>ReLU output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br><span class="hljs-comment"># nn.MaxPool2d(kernel_size=3, stride=2)</span><br>MaxPool2d output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>])<br>Flatten output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">6400</span>])<br><span class="hljs-comment"># nn.Linear(6400, 4096)</span><br>Linear output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>ReLU output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>Dropout output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br><span class="hljs-comment"># nn.Linear(4096, 4096)</span><br>Linear output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>ReLU output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br><span class="hljs-comment"># nn.Linear(4096, 10)</span><br>Dropout output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>Linear output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure>
<h2 id="vgg-使用块的网络">VGG 使用块的网络</h2>
<h3 id="特点-1">特点</h3>
<p><strong>更深更窄</strong></p>
<ul>
<li>堆更多的<code>3 x 3</code>的卷积核</li>
</ul>
<p><strong>VGG块</strong></p>
<ul>
<li><code>n</code>层，<code>m</code>通道的卷积层</li>
<li>最后加个<code>2 x 2</code>的最大池化层</li>
</ul>
<p><strong>VGG架构</strong></p>
<ul>
<li><code>AlexNet</code>整个卷积层的架构替换为<code>n</code>个<code>VGG</code>块，<strong>VGG-16</strong>，<strong>VGG-19</strong></li>
<li>块可重复使用，不同的卷积块个数和超参数可以得到不同复杂程度的变种</li>
</ul>
<figure>
<img src="/img/nn_Sequential.png" srcset="/img/loading.gif" lazyload alt="nn.Sequential()" />
<figcaption aria-hidden="true">nn.Sequential()</figcaption>
</figure>
<h3 id="代码-2">代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vgg_block</span>(<span class="hljs-params">num_convs,in_channels,out_channels</span>):<br>    layers = []<br>    <span class="hljs-comment"># 单纯用个for循环，不需要循环变量</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_convs):<br>        layers.append(nn.Conv2d(<br>        in_channels,out_channels,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>))<br>        layers.append(nn.ReLU())<br>        in_channels = out_channels<br>    layers.append(nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>))<br>        <span class="hljs-comment"># nn.Sequential() 接受orderDict或者Module数据</span><br>        <span class="hljs-comment"># layers为 List类型数据 前面加上*表示将List中内容解包</span><br>        <span class="hljs-comment"># 相当于把一个一个模型拿出来，放到Sequential中</span><br>    <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">conv_arch = ((<span class="hljs-number">1</span>,<span class="hljs-number">64</span>),(<span class="hljs-number">1</span>,<span class="hljs-number">128</span>),(<span class="hljs-number">2</span>,<span class="hljs-number">256</span>),(<span class="hljs-number">2</span>,<span class="hljs-number">512</span>),(<span class="hljs-number">2</span>,<span class="hljs-number">512</span>))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vgg</span>(<span class="hljs-params">conv_arch</span>):<br>    conv_blks = []<br>    in_channels = <span class="hljs-number">1</span><br>    <span class="hljs-comment"># 卷积层部分</span><br>    <span class="hljs-keyword">for</span> (num_convs, out_channels) <span class="hljs-keyword">in</span> conv_arch:<br>        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))<br>        in_channels = out_channels<br><br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>        *conv_blks, nn.Flatten(),<br>        <span class="hljs-comment"># 全连接层部分</span><br>        nn.Linear(out_channels * <span class="hljs-number">7</span> * <span class="hljs-number">7</span>, <span class="hljs-number">4096</span>), nn.ReLU(), nn.Dropout(<span class="hljs-number">0.5</span>),<br>        nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>), nn.ReLU(), nn.Dropout(<span class="hljs-number">0.5</span>),<br>        nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>))<br><br>net = vgg(conv_arch)<br><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.randn(size=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>))<br><span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> net:<br>    X = blk(X)<br>    <span class="hljs-built_in">print</span>(blk.__class__.__name__,<span class="hljs-string">&#x27;output shape:\t&#x27;</span>,X.shape)<br><br><span class="hljs-comment"># 每个Sequential VGG块的作用，通道数加倍，高宽减半 </span><br>Sequential output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">112</span>, <span class="hljs-number">112</span>])<br>Sequential output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">56</span>, <span class="hljs-number">56</span>])<br>Sequential output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br>Sequential output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>])<br>Sequential output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>])<br>Flatten output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">25088</span>])<br>Linear output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>ReLU output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>Dropout output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>Linear output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>ReLU output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>Dropout output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>Linear output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 减小批次</span><br>ratio = <span class="hljs-number">4</span><br>small_conv_arch = [(pair[<span class="hljs-number">0</span>], pair[<span class="hljs-number">1</span>] // ratio) <span class="hljs-keyword">for</span> pair <span class="hljs-keyword">in</span> conv_arch]<br>net = vgg(small_conv_arch)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs, batch_size = <span class="hljs-number">0.05</span>, <span class="hljs-number">10</span>, <span class="hljs-number">128</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">224</span>)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure>
<h2 id="nin-神经网络">NiN 神经网络</h2>
<h3 id="原因-2">原因</h3>
<ul>
<li>卷积层参数个数: <span class="math inline">\(c_i \times c_o \times
k^2\)</span></li>
<li>卷积层后第一个全连接层参数个数:
<ul>
<li>相当于用<code>output_channels</code>个<span
class="math inline">\(1\times1\)</span>的卷积核进行卷积计算</li>
<li><span class="math inline">\(c_i \times c_o \times k^2\)</span></li>
</ul></li>
</ul>
<h3 id="块架构">块架构</h3>
<figure>
<img src="/img/VGGvsNiN.png" srcset="/img/loading.gif" lazyload alt="VGG vs NiN" />
<figcaption aria-hidden="true">VGG vs NiN</figcaption>
</figure>
<ul>
<li>一个卷积层后跟两个全连接层
<ul>
<li>步幅为1，无填充，输出形状跟卷积层输出一样</li>
<li>起到全连接层的作用</li>
</ul></li>
</ul>
<h3 id="特点-2">特点</h3>
<ul>
<li>无全连接层</li>
<li>交替使用NiN块和步幅为2的最大池化层
<ul>
<li>逐步减少高宽(减半)和增大通道数(加倍)</li>
</ul></li>
<li>最后使用全局平均池化层得到输出
<ul>
<li>输入通道数是类别数</li>
<li>池化层的高宽=原始输入的高宽</li>
</ul></li>
</ul>
<h3 id="代码-3">代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-comment"># 定义NiN块结构</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">nin_block</span>(<span class="hljs-params">in_channels,out_channels,kernel_size,stride,padding</span>):<br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>    nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding),nn.ReLU(),<br>    nn.Conv2d(out_channels,out_channels,kernel_size=<span class="hljs-number">1</span>),nn.ReLU(),<br>    nn.Conv2d(out_channels,out_channels,kernel_size=<span class="hljs-number">1</span>),nn.ReLU(),<br>    )<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 构建网络序列</span><br>net  = nn.Sequential(<br>    nin_block(<span class="hljs-number">1</span>,<span class="hljs-number">96</span>,kernel_size=<span class="hljs-number">11</span>,stride=<span class="hljs-number">4</span>,padding=<span class="hljs-number">0</span>),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    nin_block(<span class="hljs-number">96</span>,<span class="hljs-number">256</span>,kernel_size=<span class="hljs-number">5</span>,stride=<span class="hljs-number">1</span>,padding=<span class="hljs-number">2</span>),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    nin_block(<span class="hljs-number">256</span>,<span class="hljs-number">384</span>,kernel_size=<span class="hljs-number">3</span>,stride=<span class="hljs-number">1</span>,padding=<span class="hljs-number">1</span>),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Dropout(<span class="hljs-number">0.5</span>),<br>    nin_block(<span class="hljs-number">384</span>,<span class="hljs-number">10</span>,kernel_size=<span class="hljs-number">3</span>,stride=<span class="hljs-number">1</span>,padding=<span class="hljs-number">1</span>),<br>    nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>    <span class="hljs-comment"># 把数据拉成 batch_size x 10 </span><br>    nn.Flatten()<br>)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 批次 1 大小 1 x 224 x 224</span><br>X = torch.rand(size=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>))<br><span class="hljs-comment"># 打印网络信息</span><br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X = layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">&#x27;output:\t&#x27;</span>,X.shape)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 训练</span><br>lr, num_epochs, batch_size = <span class="hljs-number">0.1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">128</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">224</span>)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure>
<h2 id="googlenet-网络">GoogLeNet 网络</h2>
<h3 id="特点-3">特点</h3>
<ul>
<li><p>引入了<code>Inception</code>块</p>
<figure>
<img src="/img/googLeNet_Inception.png" srcset="/img/loading.gif" lazyload alt="Inception Block" />
<figcaption aria-hidden="true">Inception Block</figcaption>
</figure>
<p><code>Inception</code>块由四条并行路径组成。
前三条路径使用窗口大小为1×1、3×3和5×5的卷积层，从不同空间大小中提取信息。
中间的两条路径在输入上执行1×1卷积，以减少通道数，从而降低模型的复杂性。
第四条路径使用3×3最大汇聚层，然后使用1×1卷积层来改变通道数。
这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成<code>Inception</code>块的输出。在<code>Inception</code>块中，通常调整的超参数是每层输出通道数。</p></li>
<li><p>基于<code>Inception</code>块，构建了网络</p>
<figure>
<img src="/img/inception-full.svg" srcset="/img/loading.gif" lazyload alt="GooLeNet架构" />
<figcaption aria-hidden="true">GooLeNet架构</figcaption>
</figure></li>
</ul>
<h3 id="代码-4">代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-comment"># 定义Inception块</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Inception</span>(nn.Module):<br>    <span class="hljs-comment"># c1--c4是每条路径的输出通道数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Inception, self).__init__(**kwargs)<br>        <span class="hljs-comment"># 线路1，单1x1卷积层</span><br>        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 线路2，1x1卷积层后接3x3卷积层</span><br>        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p2_2 = nn.Conv2d(c2[<span class="hljs-number">0</span>], c2[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 线路3，1x1卷积层后接5x5卷积层</span><br>        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p3_2 = nn.Conv2d(c3[<span class="hljs-number">0</span>], c3[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span><br>        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br>        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        p1 = F.relu(self.p1_1(x))<br>        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))<br>        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))<br>        p4 = F.relu(self.p4_2(self.p4_1(x)))<br>        <span class="hljs-comment"># 在通道维度上连结输出</span><br>        <span class="hljs-keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义网络的各个部分</span><br>b1 = nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">64</span>,kernel_size=<span class="hljs-number">7</span>,stride=<span class="hljs-number">2</span>,padding=<span class="hljs-number">3</span>),<br>    nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>,stride=<span class="hljs-number">2</span>,padding=<span class="hljs-number">1</span>)<br>)<br><br>b2 = nn.Sequential(nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">1</span>),<br>                   nn.ReLU(),<br>                   nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>                   nn.ReLU(),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b3 = nn.Sequential(Inception(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">32</span>), <span class="hljs-number">32</span>),<br>                   Inception(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">192</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">96</span>), <span class="hljs-number">64</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b4 = nn.Sequential(Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">208</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">48</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, (<span class="hljs-number">112</span>, <span class="hljs-number">224</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, (<span class="hljs-number">144</span>, <span class="hljs-number">288</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><br>b5 = nn.Sequential(Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, (<span class="hljs-number">192</span>, <span class="hljs-number">384</span>), (<span class="hljs-number">48</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>                   nn.Flatten())<br><br>net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 用小批量数据打印网络信息</span><br>X = torch.rand(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">96</span>, <span class="hljs-number">96</span>))<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X = layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br><br>Sequential output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>])<br>Sequential output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">192</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br>Sequential output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">480</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>])<br>Sequential output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">832</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br>Sequential output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1024</span>])<br>Linear output shape:	 torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#训练</span><br>lr, num_epochs, batch_size = <span class="hljs-number">0.1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">128</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">96</span>)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure>
<h2 id="resnet-残差网络">ResNet 残差网络</h2>
<h3 id="特点-4">特点</h3>
<p>保证新加入的层的使得效果至少不会变差</p>
<h3 id="残差块">残差块</h3>
<ul>
<li>串联一个层改变函数类，扩大函数类</li>
<li>残差块加入快速通道得到<span class="math inline">\(f(x) = x +
g(x)\)</span>的结构</li>
</ul>
<figure>
<img src="/img/resnet-block.svg" srcset="/img/loading.gif" lazyload alt="resnet-block" />
<figcaption aria-hidden="true">resnet-block</figcaption>
</figure>
<p>原来的向后传播，或者加入一个1x1卷积层来变换通道数量使得能够继续向后</p>
<ul>
<li>即使中间的块什么都没有学到，之前的层的结果还是能够继续向后传播</li>
<li>类似VGG和GoogLeNet的总体架构，但是替换成了ResNet块</li>
</ul>
<h4 id="种类">种类</h4>
<ul>
<li>高宽减半<code>ResNet</code>块（步幅为2）</li>
<li>后接多个高宽不变<code>ResNet</code>块</li>
</ul>
<h3 id="代码-5">代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><br><span class="hljs-comment"># 定义Residual块</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Residual</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,input_channels,num_channels,use_1x1conv=<span class="hljs-literal">False</span>,strides=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1 = nn.Conv2d(input_channels,num_channels,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>,stride=strides)<br>        self.conv2 = nn.Conv2d(num_channels,num_channels,kernel_size=<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-keyword">if</span> use_1x1conv:<br>            self.conv3 = nn.Conv2d(input_channels,num_channels,kernel_size=<span class="hljs-number">1</span>,stride=strides)<br>        <span class="hljs-keyword">else</span>:<br>            self.conv3 = <span class="hljs-literal">None</span><br>        self.bn1 = nn.BatchNorm2d(num_channels)<br>        self.bn2 = nn.BatchNorm2d(num_channels)<br>        <span class="hljs-comment"># inplace直接操作input，不另外开辟内存进ReLU操作</span><br>        <span class="hljs-comment"># 原地更新</span><br>        self.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,X</span>):<br>        Y = F.relu(self.bn1(self.conv1(X)))<br>        Y = self.bn2(self.conv2(Y))<br>        <span class="hljs-keyword">if</span> self.conv3:<br>            X = self.conv3(X)<br>        Y += X<br>        <br>        <span class="hljs-keyword">return</span> F.relu(Y)<br>    <br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-comment">#将块组成模块，各个模块构成ResNet</span><br><br>b1 = nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">64</span>,kernel_size=<span class="hljs-number">7</span>,stride=<span class="hljs-number">2</span>),<br>    nn.BatchNorm2d(<span class="hljs-number">64</span>),nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>,stride=<span class="hljs-number">2</span>,padding=<span class="hljs-number">1</span>)<br>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet_block</span>(<span class="hljs-params">input_channels,num_channels,num_residuals,first_block=<span class="hljs-literal">False</span></span>):<br>    blk = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_residuals):<br>        <span class="hljs-keyword">if</span> i==<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> first_block:<br>            blk.append(Residual(input_channels,num_channels,use_1x1conv=<span class="hljs-literal">True</span>,strides=<span class="hljs-number">2</span>))<br>        <span class="hljs-keyword">else</span>:<br>            blk.append(Residual(num_channels,num_channels))<br>    <span class="hljs-keyword">return</span> blk<br><br><span class="hljs-comment"># *字典解包</span><br>b2 = nn.Sequential(*resnet_block(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>,<span class="hljs-number">2</span>,first_block=<span class="hljs-literal">True</span>))<br>b3 = nn.Sequential(*resnet_block(<span class="hljs-number">64</span>,<span class="hljs-number">128</span>,<span class="hljs-number">2</span>))<br>b4 = nn.Sequential(*resnet_block(<span class="hljs-number">128</span>,<span class="hljs-number">256</span>,<span class="hljs-number">2</span>))<br>b5 = nn.Sequential(*resnet_block(<span class="hljs-number">256</span>,<span class="hljs-number">512</span>,<span class="hljs-number">2</span>))<br><br>net = nn.Sequential(b1,b2,b3,b4,b5,<br>                    nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>                    nn.Flatten(),nn.Linear(<span class="hljs-number">512</span>,<span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs, batch_size = <span class="hljs-number">0.05</span>, <span class="hljs-number">10</span>, <span class="hljs-number">256</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">96</span>)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure>
<h3 id="总结-4">总结</h3>
<ul>
<li>残差块使得很深的网络更加容易训练</li>
<li>后面的网络或多或少都有残差块的思想</li>
<li>下面小的先训练好，再训练深的，因为有跳转层</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Notes/" class="category-chain-item">Notes</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/DeepLearning/">#DeepLearning</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>深度学习 卷积 笔记</div>
      <div>https://anonymouslosty.github.io/2023/06/03/深度学习 卷积 笔记/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Ling yi</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年6月3日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2023年6月15日</div>
        </div>
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/06/15/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20GPU%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%BA%A6%E7%AC%94%E8%AE%B0/" title="深度学习 GPU与模型调度 笔记">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">深度学习 GPU与模型调度 笔记</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/06/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%20%E7%AC%94%E8%AE%B0/" title="深度学习 神经网络基础 笔记">
                        <span class="hidden-mobile">深度学习 神经网络基础 笔记</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":true,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
